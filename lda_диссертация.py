# -*- coding: utf-8 -*-
"""LDA_Диссертация.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/103ctORzn7rrqFi_LTtTe7zpo0qoRfLbv

#CHINA
import warnings
warnings.filterwarnings("ignore")

import os
import pandas as pd
import gensim
import pyLDAvis.gensim_models
import matplotlib.pyplot as plt
import numpy as np
import random

from gensim import corpora
from gensim.models import CoherenceModel
from gensim.models.ldamulticore import LdaMulticore
from tqdm import tqdm
from sklearn.metrics import pairwise_distances
from wordcloud import WordCloud
from pyvis.network import Network  # 新增网络图库

np.random.seed(1234)  # 设置numpy的随机种子
random.seed(1234)     # 设置python的随机种子

# 步骤2：配置参数（保持原参数不变）
INPUT_PATH = "/content/CN.csv"  # 输入数据路径
OUTPUT_DIR = "/content/lda_output_CN/CN2_15_R1234P50"  # 输出结果路径

PARAMS = {
    "CHINA": {
        "TOPIC_RANGE": (2, 15),
        "DYNAMIC_ALPHA": "asymmetric",
        "DYNAMIC_ETA": "auto",
        "PASSES": 50,
        "ITERATIONS": 1000,
        "WORKERS": 4,
        "WORD2VEC_FILTER": True
    }
}

FILTER_SETTINGS = {
    "CHINA": {
        "NO_BELOW": 4,
        "NO_ABOVE": 0.53,
        "KEEP_N": 3000,
        "MIN_DOC_LENGTH": 6
    }
}

# 步骤3：数据加载
def load_data(path, lang):
    '''加载并预处理数据'''
    df = pd.read_csv(path, encoding='utf-8', engine='python')
    # 输出列名以便调试
    print(df.columns)
    # 假设文本列名为 'tokens'
    raw_docs = df['tokens'].dropna().tolist()
    # 将日期转为 datetime 类型
    df['date'] = pd.to_datetime(df['date'], errors='coerce')
    # 处理 tokens 列中的每个文档，确保每个文档至少包含一定数量的单词
    return [str(doc).split() for doc in raw_docs if len(str(doc).split()) >= FILTER_SETTINGS[lang]["MIN_DOC_LENGTH"]], df

# 步骤4：词向量训练
def train_word2vec(docs, lang):
    '''训练词向量模型'''
    return gensim.models.Word2Vec(
        sentences=docs,
        vector_size=100,
        window=5,
        min_count=2,
        workers=PARAMS[lang]["WORKERS"]
    )

# 步骤5：语料库构建
def build_corpus(docs, lang):
    '''构建优化语料库'''
    # 词向量语义过滤
    if PARAMS[lang]["WORD2VEC_FILTER"]:
        w2v_model = train_word2vec(docs, lang)
        vocab = set(w2v_model.wv.index_to_key)
        docs = [[word for word in doc if word in vocab] for doc in docs]

    # 构建词典
    dictionary = corpora.Dictionary(docs)
    dictionary.filter_extremes(
        no_below=FILTER_SETTINGS[lang]["NO_BELOW"],
        no_above=FILTER_SETTINGS[lang]["NO_ABOVE"],
        keep_n=FILTER_SETTINGS[lang]["KEEP_N"]
    )
    return dictionary, list(dictionary.doc2bow(doc) for doc in docs)

# 步骤6：模型评估（一致性和困惑度）
def calculate_metrics(model, corpus, docs, dictionary):
    '''计算一致性和困惑度'''
    coherence = CoherenceModel(model, texts=docs, dictionary=dictionary, coherence='c_v').get_coherence()
    perplexity = model.log_perplexity(corpus)
    return coherence, perplexity

# 步骤7：主题强度随时间演变趋势图
def plot_topic_trends(df, model, corpus, dictionary, lang):
    '''绘制主题强度随时间演变的趋势图'''
    df['year'] = df['date'].dt.year
    df['month'] = df['date'].dt.month
    df['topic_strength'] = 0

    topic_strength_by_year = []

    for year in range(2012, 2025):
        yearly_docs = df[df['year'] == year]
        topic_strength_year = []

        for _, doc in yearly_docs.iterrows():
            doc_bow = dictionary.doc2bow(doc['tokens'].split())
            topic_dist = model.get_document_topics(doc_bow, minimum_probability=0)
            topic_strength = {f"Topic_{tid + 1}": prob for tid, prob in topic_dist}
            topic_strength_year.append(topic_strength)

        # 计算每个主题的强度
        if topic_strength_year:
            df_topic_strength = pd.DataFrame(topic_strength_year)
            avg_topic_strength = df_topic_strength.mean() * 100
            topic_strength_by_year.append(avg_topic_strength)

    topic_strength_by_year = pd.DataFrame(topic_strength_by_year, index=range(2013, 2025))

    # 绘制主题强度随时间变化的趋势图
    plt.figure(figsize=(12, 8))
    for topic in topic_strength_by_year.columns:
        plt.plot(topic_strength_by_year.index, topic_strength_by_year[topic], label=topic)

    plt.xlabel('Year')
    plt.ylabel('Topic Strength (%)')
    plt.title(f"Topic Strength Over Time ({lang})")
    plt.legend()
    plt.grid(True)
    plt.show()  # 直接显示图形


# 步骤8：保存模型输出
def save_outputs(model, corpus, dictionary, lang, metrics, df):
    '''保存所有输出结果'''
    save_path = os.path.join(OUTPUT_DIR, lang)

    # 确保保存路径存在
    os.makedirs(save_path, exist_ok=True)

    # 子步骤8.1：文档-主题矩阵
    doc_topic_list = []
    for doc_id, doc in enumerate(corpus):
        topic_dist = model.get_document_topics(doc, minimum_probability=0)
        doc_data = {"Document_ID": doc_id}
        doc_data.update({f"Topic_{tid + 1}": prob for tid, prob in topic_dist})
        doc_topic_list.append(doc_data)
    pd.DataFrame(doc_topic_list).to_excel(os.path.join(save_path, f"{lang}_document_topic_matrix.xlsx"), index=False)

    # 子步骤8.2：主题强度计算
    df_doc_topic = pd.DataFrame(doc_topic_list)
    topic_strength = df_doc_topic.drop("Document_ID", axis=1).mean() * 100
    pd.DataFrame({
        "Topic": topic_strength.index,
        "Strength(%)": topic_strength.values.round(2)
    }).to_excel(os.path.join(save_path, f"{lang}_topic_strength.xlsx"), index=False)

    # 子步骤8.3：主题词分布（30词）
    topics_data = []
    for topic_id in range(model.num_topics):
        words = model.show_topic(topic_id, topn=30)
        topics_data.append({
            "Topic": topic_id + 1,
            "Keywords": " | ".join([f"{w[0]}({w[1]:.3f})" for w in words])
        })
    pd.DataFrame(topics_data).to_excel(os.path.join(save_path, f"{lang}_topics.xlsx"), index=False)

    # 子步骤8.4：生成词云（语料库词云）
    all_words = sum([doc.split() for doc in df['tokens']], [])
    wordcloud = WordCloud(width=1200, height=800, background_color='white').generate(' '.join(all_words))
    plt.figure(figsize=(15, 10))
    plt.imshow(wordcloud)
    plt.axis("off")
    plt.title(f"Word Cloud for {lang}", fontsize=16)
    plt.show()  # 直接显示图形


    # 子步骤8.5：生成气泡图（pyLDAvis）
    vis = pyLDAvis.gensim_models.prepare(model, corpus, dictionary)
    pyLDAvis.save_html(vis, os.path.join(save_path, f"{lang}_bubble.html"))

    # 子步骤8.6：生成词关系网络图（pyvis）
    net = Network(notebook=True)
    for topic_id in range(model.num_topics):
        words = model.show_topic(topic_id, topn=30)
        for word, weight in words:
            net.add_node(word, label=word, title=f"Weight: {weight:.3f}")
    for topic_id in range(model.num_topics):
        words = model.show_topic(topic_id, topn=30)
        for i in range(len(words)):
            for j in range(i + 1, len(words)):
                net.add_edge(words[i][0], words[j][0], weight=0.5)  # 连接高权重的词

    net.save_graph(os.path.join(save_path, f"{lang}_word_network.html"))

    # 子步骤8.7：主题强度随时间变化的趋势图
    plot_topic_trends(df, model, corpus, dictionary, lang)

    # 子步骤8.8：评估曲线
    plt.figure(figsize=(15, 5))
    x = range(*PARAMS[lang]["TOPIC_RANGE"])
    plt.subplot(131)
    plt.plot(x, metrics['coherence'], 'b-o')
    plt.xlabel("Number of Topics")
    plt.ylabel("Coherence Score")
    plt.title(f"{lang} Topic Coherence")

    plt.subplot(132)
    plt.plot(x, metrics['perplexity'], 'r-^')
    plt.xlabel("Number of Topics")
    plt.ylabel("Perplexity")
    plt.title(f"{lang} Perplexity")

    plt.tight_layout()
    plt.show()  # 直接显示图形

    # 保存评估曲线图像
    plt.savefig(os.path.join(save_path, f"{lang}_metrics.png"))
    plt.close()


# 步骤9：LDA训练流程
def train_lda(docs, lang, df):
    '''增强型训练流程'''
    dictionary, corpus = build_corpus(docs, lang)

    metrics = {'coherence': [], 'perplexity': []}
    topic_range = range(*PARAMS[lang]["TOPIC_RANGE"])

    # 训练多个主题模型
    for n_topics in tqdm(topic_range, desc="Training LDA Models"):
        print(f"Training LDA model with {n_topics} topics...")
        model = LdaMulticore(
            corpus=corpus,
            id2word=dictionary,
            num_topics=n_topics,
            passes=PARAMS[lang]["PASSES"],
            iterations=PARAMS[lang]["ITERATIONS"],
            workers=PARAMS[lang]["WORKERS"],
            alpha=PARAMS[lang]["DYNAMIC_ALPHA"],
            eta=PARAMS[lang]["DYNAMIC_ETA"],
            random_state=1234  # 固定随机种子
        )

        # 计算一致性和困惑度
        coherence, perplexity = calculate_metrics(model, corpus, docs, dictionary)
        metrics['coherence'].append(coherence)
        metrics['perplexity'].append(perplexity)

        # 打印一致性和困惑度
        print(f"Coherence: {coherence}, Perplexity: {perplexity}")

    # 自动选择最佳主题数：选择一致性得分最高的主题数
    best_n_topics = topic_range[np.argmax(metrics['coherence'])]
    print(f"\nBest number of topics based on coherence score: {best_n_topics}")

    # 输出一致性得分，并显示所有主题的得分
    print("Coherence scores for different topic numbers:")
    for n, score in zip(topic_range, metrics['coherence']):
        print(f"Topics: {n}, Coherence Score: {score:.4f}")

    # 绘制一致性得分和困惑度评估曲线，并保存为 PNG 文件
    plt.figure(figsize=(15, 5))

    # 一致性得分曲线
    plt.subplot(131)
    plt.plot(topic_range, metrics['coherence'], 'b-o')
    plt.xlabel("Number of Topics")
    plt.ylabel("Coherence Score")
    plt.title(f"{lang} Topic Coherence")

    # 困惑度曲线
    plt.subplot(132)
    plt.plot(topic_range, metrics['perplexity'], 'r-^')
    plt.xlabel("Number of Topics")
    plt.ylabel("Perplexity")
    plt.title(f"{lang} Perplexity")

    plt.tight_layout()

    # 保存评估曲线图像
    eval_plot_path = os.path.join(OUTPUT_DIR, f"{lang}_metrics.png")
    plt.savefig(eval_plot_path)
    plt.show()  # 显示图形

    # 训练并保存输出
    model = LdaMulticore(
        corpus=corpus,
        id2word=dictionary,
        num_topics=best_n_topics,
        passes=PARAMS[lang]["PASSES"],
        iterations=PARAMS[lang]["ITERATIONS"],
        workers=PARAMS[lang]["WORKERS"],
        alpha=PARAMS[lang]["DYNAMIC_ALPHA"],
        eta=PARAMS[lang]["DYNAMIC_ETA"]
    )
    save_outputs(model, corpus, dictionary, lang, metrics, df)


# 步骤10：主流程
def main():
    '''执行流程'''
    lang = "CHINA"  # 语言选择
    docs, df = load_data(INPUT_PATH, lang)

    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # 训练LDA模型
    train_lda(docs, lang, df)

    print("LDA model training and evaluation complete!")

if __name__ == "__main__":
    main()
"""

#RUSSIA
import warnings
warnings.filterwarnings("ignore")

import os
import pandas as pd
import gensim
import pyLDAvis.gensim_models
import matplotlib.pyplot as plt
import numpy as np
import random

from gensim import corpora
from gensim.models import CoherenceModel
from gensim.models.ldamulticore import LdaMulticore
from tqdm import tqdm
from sklearn.metrics import pairwise_distances
from wordcloud import WordCloud
from pyvis.network import Network  # 新增网络图库

np.random.seed(3407)  # 设置numpy的随机种子
random.seed(3407)     # 设置python的随机种子

# 步骤2：配置参数（保持原参数不变）
INPUT_PATH = "/content/RU.csv"  # 输入数据路径
OUTPUT_DIR = "/content/lda_output_RU/RU2_15_R3407P50"  # 输出结果路径

PARAMS = {
    "RUSSIA": {
        "TOPIC_RANGE": (2, 15),
        "DYNAMIC_ALPHA": "asymmetric",
        "DYNAMIC_ETA": "auto",
        "PASSES": 50,
        "ITERATIONS": 1000,
        "WORKERS": 4,
        "WORD2VEC_FILTER": True
    }
}

FILTER_SETTINGS = {
    "RUSSIA": {
        "NO_BELOW": 4,
        "NO_ABOVE": 0.53,
        "KEEP_N": 3000,
        "MIN_DOC_LENGTH": 6
    }
}

# 步骤3：数据加载
def load_data(path, lang):
    """加载并预处理数据"""
    df = pd.read_csv(path, encoding='utf-8', engine='python')
    # 输出列名以便调试
    print(df.columns)
    # 假设文本列名为 'tokens'
    raw_docs = df['tokens'].dropna().tolist()
    # 将日期转为 datetime 类型
    df['date'] = pd.to_datetime(df['date'], errors='coerce')
    # 处理 tokens 列中的每个文档，确保每个文档至少包含一定数量的单词
    return [str(doc).split() for doc in raw_docs if len(str(doc).split()) >= FILTER_SETTINGS[lang]["MIN_DOC_LENGTH"]], df

# 步骤4：词向量训练
def train_word2vec(docs, lang):
    """训练词向量模型"""
    return gensim.models.Word2Vec(
        sentences=docs,
        vector_size=100,
        window=5,
        min_count=2,
        workers=PARAMS[lang]["WORKERS"]
    )

# 步骤5：语料库构建
def build_corpus(docs, lang):
    """构建优化语料库"""
    # 词向量语义过滤
    if PARAMS[lang]["WORD2VEC_FILTER"]:
        w2v_model = train_word2vec(docs, lang)
        vocab = set(w2v_model.wv.index_to_key)
        docs = [[word for word in doc if word in vocab] for doc in docs]

    # 构建词典
    dictionary = corpora.Dictionary(docs)
    dictionary.filter_extremes(
        no_below=FILTER_SETTINGS[lang]["NO_BELOW"],
        no_above=FILTER_SETTINGS[lang]["NO_ABOVE"],
        keep_n=FILTER_SETTINGS[lang]["KEEP_N"]
    )
    return dictionary, list(dictionary.doc2bow(doc) for doc in docs)

# 步骤6：模型评估（一致性和困惑度）
def calculate_metrics(model, corpus, docs, dictionary):
    """计算一致性和困惑度"""
    coherence = CoherenceModel(model, texts=docs, dictionary=dictionary, coherence='c_v').get_coherence()
    perplexity = model.log_perplexity(corpus)
    return coherence, perplexity

# 步骤7：主题强度随时间演变趋势图
def plot_topic_trends(df, model, corpus, dictionary, lang):
    """绘制主题强度随时间演变的趋势图"""
    df['year'] = df['date'].dt.year
    df['month'] = df['date'].dt.month
    df['topic_strength'] = 0

    topic_strength_by_year = []

    for year in range(2012, 2025):
        yearly_docs = df[df['year'] == year]
        topic_strength_year = []

        for _, doc in yearly_docs.iterrows():
            doc_bow = dictionary.doc2bow(doc['tokens'].split())
            topic_dist = model.get_document_topics(doc_bow, minimum_probability=0)
            topic_strength = {f"Topic_{tid + 1}": prob for tid, prob in topic_dist}
            topic_strength_year.append(topic_strength)

        # 计算每个主题的强度
        if topic_strength_year:
            df_topic_strength = pd.DataFrame(topic_strength_year)
            avg_topic_strength = df_topic_strength.mean() * 100
            topic_strength_by_year.append(avg_topic_strength)

    topic_strength_by_year = pd.DataFrame(topic_strength_by_year, index=range(2013, 2025))

    # 绘制主题强度随时间变化的趋势图
    plt.figure(figsize=(12, 8))
    for topic in topic_strength_by_year.columns:
        plt.plot(topic_strength_by_year.index, topic_strength_by_year[topic], label=topic)

    plt.xlabel('Year')
    plt.ylabel('Topic Strength (%)')
    plt.title(f"Topic Strength Over Time ({lang})")
    plt.legend()
    plt.grid(True)
    plt.show()  # 直接显示图形


# 步骤8：保存模型输出
def save_outputs(model, corpus, dictionary, lang, metrics, df):
    """保存所有输出结果"""
    save_path = os.path.join(OUTPUT_DIR, lang)

    # 确保保存路径存在
    os.makedirs(save_path, exist_ok=True)

    # 子步骤8.1：文档-主题矩阵
    doc_topic_list = []
    for doc_id, doc in enumerate(corpus):
        topic_dist = model.get_document_topics(doc, minimum_probability=0)
        doc_data = {"Document_ID": doc_id}
        doc_data.update({f"Topic_{tid + 1}": prob for tid, prob in topic_dist})
        doc_topic_list.append(doc_data)
    pd.DataFrame(doc_topic_list).to_excel(os.path.join(save_path, f"{lang}_document_topic_matrix.xlsx"), index=False)

    # 子步骤8.2：主题强度计算
    df_doc_topic = pd.DataFrame(doc_topic_list)
    topic_strength = df_doc_topic.drop("Document_ID", axis=1).mean() * 100
    pd.DataFrame({
        "Topic": topic_strength.index,
        "Strength(%)": topic_strength.values.round(2)
    }).to_excel(os.path.join(save_path, f"{lang}_topic_strength.xlsx"), index=False)

    # 子步骤8.3：主题词分布（30词）
    topics_data = []
    for topic_id in range(model.num_topics):
        words = model.show_topic(topic_id, topn=30)
        topics_data.append({
            "Topic": topic_id + 1,
            "Keywords": " | ".join([f"{w[0]}({w[1]:.3f})" for w in words])
        })
    pd.DataFrame(topics_data).to_excel(os.path.join(save_path, f"{lang}_topics.xlsx"), index=False)

    # 子步骤8.4：生成词云（语料库词云）
    all_words = sum([doc.split() for doc in df['tokens']], [])
    wordcloud = WordCloud(width=1200, height=800, background_color='white').generate(' '.join(all_words))
    plt.figure(figsize=(15, 10))
    plt.imshow(wordcloud)
    plt.axis("off")
    plt.title(f"Word Cloud for {lang}", fontsize=16)
    plt.show()  # 直接显示图形


    # 子步骤8.5：生成气泡图（pyLDAvis）
    vis = pyLDAvis.gensim_models.prepare(model, corpus, dictionary)
    pyLDAvis.save_html(vis, os.path.join(save_path, f"{lang}_bubble.html"))

    # 子步骤8.6：生成词关系网络图（pyvis）
    net = Network(notebook=True)
    for topic_id in range(model.num_topics):
        words = model.show_topic(topic_id, topn=30)
        for word, weight in words:
            net.add_node(word, label=word, title=f"Weight: {weight:.3f}")
    for topic_id in range(model.num_topics):
        words = model.show_topic(topic_id, topn=30)
        for i in range(len(words)):
            for j in range(i + 1, len(words)):
                net.add_edge(words[i][0], words[j][0], weight=0.5)  # 连接高权重的词

    net.save_graph(os.path.join(save_path, f"{lang}_word_network.html"))

    # 子步骤8.7：主题强度随时间变化的趋势图
    plot_topic_trends(df, model, corpus, dictionary, lang)

    # 子步骤8.8：评估曲线
    plt.figure(figsize=(15, 5))
    x = range(*PARAMS[lang]["TOPIC_RANGE"])
    plt.subplot(131)
    plt.plot(x, metrics['coherence'], 'b-o')
    plt.xlabel("Number of Topics")
    plt.ylabel("Coherence Score")
    plt.title(f"{lang} Topic Coherence")

    plt.subplot(132)
    plt.plot(x, metrics['perplexity'], 'r-^')
    plt.xlabel("Number of Topics")
    plt.ylabel("Perplexity")
    plt.title(f"{lang} Perplexity")

    plt.tight_layout()
    plt.show()  # 直接显示图形

    # 保存评估曲线图像
    plt.savefig(os.path.join(save_path, f"{lang}_metrics.png"))
    plt.close()


# 步骤9：LDA训练流程
def train_lda(docs, lang, df):
    """增强型训练流程"""
    dictionary, corpus = build_corpus(docs, lang)

    metrics = {'coherence': [], 'perplexity': []}
    topic_range = range(*PARAMS[lang]["TOPIC_RANGE"])

    # 训练多个主题模型
    for n_topics in tqdm(topic_range, desc="Training LDA Models"):
        print(f"Training LDA model with {n_topics} topics...")
        model = LdaMulticore(
            corpus=corpus,
            id2word=dictionary,
            num_topics=n_topics,
            passes=PARAMS[lang]["PASSES"],
            iterations=PARAMS[lang]["ITERATIONS"],
            workers=PARAMS[lang]["WORKERS"],
            alpha=PARAMS[lang]["DYNAMIC_ALPHA"],
            eta=PARAMS[lang]["DYNAMIC_ETA"],
            random_state=3407  # 固定随机种子
        )

        # 计算一致性和困惑度
        coherence, perplexity = calculate_metrics(model, corpus, docs, dictionary)
        metrics['coherence'].append(coherence)
        metrics['perplexity'].append(perplexity)

        # 打印一致性和困惑度
        print(f"Coherence: {coherence}, Perplexity: {perplexity}")

    # 自动选择最佳主题数：选择一致性得分最高的主题数
    best_n_topics = topic_range[np.argmax(metrics['coherence'])]
    print(f"\nBest number of topics based on coherence score: {best_n_topics}")

    # 输出一致性得分，并显示所有主题的得分
    print("Coherence scores for different topic numbers:")
    for n, score in zip(topic_range, metrics['coherence']):
        print(f"Topics: {n}, Coherence Score: {score:.4f}")

    # 绘制一致性得分和困惑度评估曲线，并保存为 PNG 文件
    plt.figure(figsize=(15, 5))

    # 一致性得分曲线
    plt.subplot(131)
    plt.plot(topic_range, metrics['coherence'], 'b-o')
    plt.xlabel("Number of Topics")
    plt.ylabel("Coherence Score")
    plt.title(f"{lang} Topic Coherence")

    # 困惑度曲线
    plt.subplot(132)
    plt.plot(topic_range, metrics['perplexity'], 'r-^')
    plt.xlabel("Number of Topics")
    plt.ylabel("Perplexity")
    plt.title(f"{lang} Perplexity")

    plt.tight_layout()

    # 保存评估曲线图像
    eval_plot_path = os.path.join(OUTPUT_DIR, f"{lang}_metrics.png")
    plt.savefig(eval_plot_path)
    plt.show()  # 显示图形

    # 训练并保存输出
    model = LdaMulticore(
        corpus=corpus,
        id2word=dictionary,
        num_topics=best_n_topics,
        passes=PARAMS[lang]["PASSES"],
        iterations=PARAMS[lang]["ITERATIONS"],
        workers=PARAMS[lang]["WORKERS"],
        alpha=PARAMS[lang]["DYNAMIC_ALPHA"],
        eta=PARAMS[lang]["DYNAMIC_ETA"]
    )
    save_outputs(model, corpus, dictionary, lang, metrics, df)


# 步骤10：主流程
def main():
    """执行流程"""
    lang = "RUSSIA"  # 语言选择
    docs, df = load_data(INPUT_PATH, lang)

    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # 训练LDA模型
    train_lda(docs, lang, df)

    print("LDA model training and evaluation complete!")

if __name__ == "__main__":
    main()

#CHINA
import warnings
warnings.filterwarnings("ignore")

import os
import pandas as pd
import gensim
import pyLDAvis.gensim_models
import matplotlib.pyplot as plt
import numpy as np
import random

from gensim import corpora
from gensim.models import CoherenceModel
from gensim.models.ldamulticore import LdaMulticore
from tqdm import tqdm
from sklearn.metrics import pairwise_distances
from wordcloud import WordCloud
from pyvis.network import Network  # 新增网络图库

np.random.seed(1234)  # 设置numpy的随机种子
random.seed(1234)     # 设置python的随机种子

# 步骤2：配置参数（保持原参数不变）
INPUT_PATH = "/content/CN.csv"  # 输入数据路径
OUTPUT_DIR = "/content/lda_output_CN/CN2_15_R1234P50"  # 输出结果路径

PARAMS = {
    "CHINA": {
        "TOPIC_RANGE": (2, 15),
        "DYNAMIC_ALPHA": "asymmetric",
        "DYNAMIC_ETA": "auto",
        "PASSES": 50,
        "ITERATIONS": 1000,
        "WORKERS": 4,
        "WORD2VEC_FILTER": True
    }
}

FILTER_SETTINGS = {
    "CHINA": {
        "NO_BELOW": 4,
        "NO_ABOVE": 0.53,
        "KEEP_N": 3000,
        "MIN_DOC_LENGTH": 6
    }
}

# 步骤3：数据加载
def load_data(path, lang):
    """加载并预处理数据"""
    df = pd.read_csv(path, encoding='utf-8', engine='python')
    # 输出列名以便调试
    print(df.columns)
    # 假设文本列名为 'tokens'
    raw_docs = df['tokens'].dropna().tolist()
    # 将日期转为 datetime 类型
    df['date'] = pd.to_datetime(df['date'], errors='coerce')
    # 处理 tokens 列中的每个文档，确保每个文档至少包含一定数量的单词
    return [str(doc).split() for doc in raw_docs if len(str(doc).split()) >= FILTER_SETTINGS[lang]["MIN_DOC_LENGTH"]], df

# 步骤4：词向量训练
def train_word2vec(docs, lang):
    """训练词向量模型"""
    return gensim.models.Word2Vec(
        sentences=docs,
        vector_size=100,
        window=5,
        min_count=2,
        workers=PARAMS[lang]["WORKERS"]
    )

# 步骤5：语料库构建
def build_corpus(docs, lang):
    """构建优化语料库"""
    # 词向量语义过滤
    if PARAMS[lang]["WORD2VEC_FILTER"]:
        w2v_model = train_word2vec(docs, lang)
        vocab = set(w2v_model.wv.index_to_key)
        docs = [[word for word in doc if word in vocab] for doc in docs]

    # 构建词典
    dictionary = corpora.Dictionary(docs)
    dictionary.filter_extremes(
        no_below=FILTER_SETTINGS[lang]["NO_BELOW"],
        no_above=FILTER_SETTINGS[lang]["NO_ABOVE"],
        keep_n=FILTER_SETTINGS[lang]["KEEP_N"]
    )
    return dictionary, list(dictionary.doc2bow(doc) for doc in docs)

# 步骤6：模型评估（一致性和困惑度）
def calculate_metrics(model, corpus, docs, dictionary):
    """计算一致性和困惑度"""
    coherence = CoherenceModel(model, texts=docs, dictionary=dictionary, coherence='c_v').get_coherence()
    perplexity = model.log_perplexity(corpus)
    return coherence, perplexity

# 步骤7：主题强度随时间演变趋势图
def plot_topic_trends(df, model, corpus, dictionary, lang):
    """绘制主题强度随时间演变的趋势图"""
    df['year'] = df['date'].dt.year
    df['month'] = df['date'].dt.month
    df['topic_strength'] = 0

    topic_strength_by_year = []

    for year in range(2012, 2025):
        yearly_docs = df[df['year'] == year]
        topic_strength_year = []

        for _, doc in yearly_docs.iterrows():
            doc_bow = dictionary.doc2bow(doc['tokens'].split())
            topic_dist = model.get_document_topics(doc_bow, minimum_probability=0)
            topic_strength = {f"Topic_{tid + 1}": prob for tid, prob in topic_dist}
            topic_strength_year.append(topic_strength)

        # 计算每个主题的强度
        if topic_strength_year:
            df_topic_strength = pd.DataFrame(topic_strength_year)
            avg_topic_strength = df_topic_strength.mean() * 100
            topic_strength_by_year.append(avg_topic_strength)

    topic_strength_by_year = pd.DataFrame(topic_strength_by_year, index=range(2013, 2025))

    # 绘制主题强度随时间变化的趋势图
    plt.figure(figsize=(12, 8))
    for topic in topic_strength_by_year.columns:
        plt.plot(topic_strength_by_year.index, topic_strength_by_year[topic], label=topic)

    plt.xlabel('Year')
    plt.ylabel('Topic Strength (%)')
    plt.title(f"Topic Strength Over Time ({lang})")
    plt.legend()
    plt.grid(True)
    plt.show()  # 直接显示图形


# 步骤8：保存模型输出
def save_outputs(model, corpus, dictionary, lang, metrics, df):
    """保存所有输出结果"""
    save_path = os.path.join(OUTPUT_DIR, lang)

    # 确保保存路径存在
    os.makedirs(save_path, exist_ok=True)

    # 子步骤8.1：文档-主题矩阵
    doc_topic_list = []
    for doc_id, doc in enumerate(corpus):
        topic_dist = model.get_document_topics(doc, minimum_probability=0)
        doc_data = {"Document_ID": doc_id}
        doc_data.update({f"Topic_{tid + 1}": prob for tid, prob in topic_dist})
        doc_topic_list.append(doc_data)
    pd.DataFrame(doc_topic_list).to_excel(os.path.join(save_path, f"{lang}_document_topic_matrix.xlsx"), index=False)

    # 子步骤8.2：主题强度计算
    df_doc_topic = pd.DataFrame(doc_topic_list)
    topic_strength = df_doc_topic.drop("Document_ID", axis=1).mean() * 100
    pd.DataFrame({
        "Topic": topic_strength.index,
        "Strength(%)": topic_strength.values.round(2)
    }).to_excel(os.path.join(save_path, f"{lang}_topic_strength.xlsx"), index=False)

    # 子步骤8.3：主题词分布（30词）
    topics_data = []
    for topic_id in range(model.num_topics):
        words = model.show_topic(topic_id, topn=30)
        topics_data.append({
            "Topic": topic_id + 1,
            "Keywords": " | ".join([f"{w[0]}({w[1]:.3f})" for w in words])
        })
    pd.DataFrame(topics_data).to_excel(os.path.join(save_path, f"{lang}_topics.xlsx"), index=False)

    # 子步骤8.4：生成词云（语料库词云）
    all_words = sum([doc.split() for doc in df['tokens']], [])
    wordcloud = WordCloud(width=1200, height=800, background_color='white').generate(' '.join(all_words))
    plt.figure(figsize=(15, 10))
    plt.imshow(wordcloud)
    plt.axis("off")
    plt.title(f"Word Cloud for {lang}", fontsize=16)
    plt.show()  # 直接显示图形


    # 子步骤8.5：生成气泡图（pyLDAvis）
    vis = pyLDAvis.gensim_models.prepare(model, corpus, dictionary)
    pyLDAvis.save_html(vis, os.path.join(save_path, f"{lang}_bubble.html"))

    # 子步骤8.6：生成词关系网络图（pyvis）
    net = Network(notebook=True)
    for topic_id in range(model.num_topics):
        words = model.show_topic(topic_id, topn=30)
        for word, weight in words:
            net.add_node(word, label=word, title=f"Weight: {weight:.3f}")
    for topic_id in range(model.num_topics):
        words = model.show_topic(topic_id, topn=30)
        for i in range(len(words)):
            for j in range(i + 1, len(words)):
                net.add_edge(words[i][0], words[j][0], weight=0.5)  # 连接高权重的词

    net.save_graph(os.path.join(save_path, f"{lang}_word_network.html"))

    # 子步骤8.7：主题强度随时间变化的趋势图
    plot_topic_trends(df, model, corpus, dictionary, lang)

    # 子步骤8.8：评估曲线
    plt.figure(figsize=(15, 5))
    x = range(*PARAMS[lang]["TOPIC_RANGE"])
    plt.subplot(131)
    plt.plot(x, metrics['coherence'], 'b-o')
    plt.xlabel("Number of Topics")
    plt.ylabel("Coherence Score")
    plt.title(f"{lang} Topic Coherence")

    plt.subplot(132)
    plt.plot(x, metrics['perplexity'], 'r-^')
    plt.xlabel("Number of Topics")
    plt.ylabel("Perplexity")
    plt.title(f"{lang} Perplexity")

    plt.tight_layout()
    plt.show()  # 直接显示图形

    # 保存评估曲线图像
    plt.savefig(os.path.join(save_path, f"{lang}_metrics.png"))
    plt.close()


# 步骤9：LDA训练流程
def train_lda(docs, lang, df):
    """增强型训练流程"""
    dictionary, corpus = build_corpus(docs, lang)

    metrics = {'coherence': [], 'perplexity': []}
    topic_range = range(*PARAMS[lang]["TOPIC_RANGE"])

    # 训练多个主题模型
    for n_topics in tqdm(topic_range, desc="Training LDA Models"):
        print(f"Training LDA model with {n_topics} topics...")
        model = LdaMulticore(
            corpus=corpus,
            id2word=dictionary,
            num_topics=n_topics,
            passes=PARAMS[lang]["PASSES"],
            iterations=PARAMS[lang]["ITERATIONS"],
            workers=PARAMS[lang]["WORKERS"],
            alpha=PARAMS[lang]["DYNAMIC_ALPHA"],
            eta=PARAMS[lang]["DYNAMIC_ETA"],
            random_state=1234  # 固定随机种子
        )

        # 计算一致性和困惑度
        coherence, perplexity = calculate_metrics(model, corpus, docs, dictionary)
        metrics['coherence'].append(coherence)
        metrics['perplexity'].append(perplexity)

        # 打印一致性和困惑度
        print(f"Coherence: {coherence}, Perplexity: {perplexity}")

    # 自动选择最佳主题数：选择一致性得分最高的主题数
    best_n_topics = topic_range[np.argmax(metrics['coherence'])]
    print(f"\nBest number of topics based on coherence score: {best_n_topics}")

    # 输出一致性得分，并显示所有主题的得分
    print("Coherence scores for different topic numbers:")
    for n, score in zip(topic_range, metrics['coherence']):
        print(f"Topics: {n}, Coherence Score: {score:.4f}")

    # 绘制一致性得分和困惑度评估曲线，并保存为 PNG 文件
    plt.figure(figsize=(15, 5))

    # 一致性得分曲线
    plt.subplot(131)
    plt.plot(topic_range, metrics['coherence'], 'b-o')
    plt.xlabel("Number of Topics")
    plt.ylabel("Coherence Score")
    plt.title(f"{lang} Topic Coherence")

    # 困惑度曲线
    plt.subplot(132)
    plt.plot(topic_range, metrics['perplexity'], 'r-^')
    plt.xlabel("Number of Topics")
    plt.ylabel("Perplexity")
    plt.title(f"{lang} Perplexity")

    plt.tight_layout()

    # 保存评估曲线图像
    eval_plot_path = os.path.join(OUTPUT_DIR, f"{lang}_metrics.png")
    plt.savefig(eval_plot_path)
    plt.show()  # 显示图形

    # 训练并保存输出
    model = LdaMulticore(
        corpus=corpus,
        id2word=dictionary,
        num_topics=best_n_topics,
        passes=PARAMS[lang]["PASSES"],
        iterations=PARAMS[lang]["ITERATIONS"],
        workers=PARAMS[lang]["WORKERS"],
        alpha=PARAMS[lang]["DYNAMIC_ALPHA"],
        eta=PARAMS[lang]["DYNAMIC_ETA"]
    )
    save_outputs(model, corpus, dictionary, lang, metrics, df)


# 步骤10：主流程
def main():
    """执行流程"""
    lang = "CHINA"  # 语言选择
    docs, df = load_data(INPUT_PATH, lang)

    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # 训练LDA模型
    train_lda(docs, lang, df)

    print("LDA model training and evaluation complete!")

if __name__ == "__main__":
    main()

#USA
import warnings
warnings.filterwarnings("ignore")

import os
import pandas as pd
import gensim
import pyLDAvis.gensim_models
import matplotlib.pyplot as plt
import numpy as np
import random

from gensim import corpora
from gensim.models import CoherenceModel
from gensim.models.ldamulticore import LdaMulticore
from tqdm import tqdm
from sklearn.metrics import pairwise_distances
from wordcloud import WordCloud
from pyvis.network import Network  # 新增网络图库

np.random.seed(3407)  # 设置numpy的随机种子
random.seed(3407)     # 设置python的随机种子

# 步骤2：配置参数（保持原参数不变）
INPUT_PATH = "/content/US.csv"  # 输入数据路径
OUTPUT_DIR = "/content/lda_output/USA2_1_R3407P50"  # 输出结果路径

PARAMS = {
    "USA": {
        "TOPIC_RANGE": (2, 15),
        "DYNAMIC_ALPHA": "asymmetric",
        "DYNAMIC_ETA": "auto",
        "PASSES": 50,
        "ITERATIONS": 1000,
        "WORKERS": 4,
        "WORD2VEC_FILTER": True
    }
}

FILTER_SETTINGS = {
    "USA": {
        "NO_BELOW": 4,
        "NO_ABOVE": 0.53,
        "KEEP_N": 3000,
        "MIN_DOC_LENGTH": 6
    }
}

# 步骤3：数据加载
def load_data(path, lang):
    """加载并预处理数据"""
    df = pd.read_csv(path, encoding='utf-8', engine='python')
    # 输出列名以便调试
    print(df.columns)
    # 假设文本列名为 'tokens'
    raw_docs = df['tokens'].dropna().tolist()
    # 将日期转为 datetime 类型
    df['date'] = pd.to_datetime(df['date'], errors='coerce')
    # 处理 tokens 列中的每个文档，确保每个文档至少包含一定数量的单词
    return [str(doc).split() for doc in raw_docs if len(str(doc).split()) >= FILTER_SETTINGS[lang]["MIN_DOC_LENGTH"]], df

# 步骤4：词向量训练
def train_word2vec(docs, lang):
    """训练词向量模型"""
    return gensim.models.Word2Vec(
        sentences=docs,
        vector_size=100,
        window=5,
        min_count=2,
        workers=PARAMS[lang]["WORKERS"]
    )

# 步骤5：语料库构建
def build_corpus(docs, lang):
    """构建优化语料库"""
    # 词向量语义过滤
    if PARAMS[lang]["WORD2VEC_FILTER"]:
        w2v_model = train_word2vec(docs, lang)
        vocab = set(w2v_model.wv.index_to_key)
        docs = [[word for word in doc if word in vocab] for doc in docs]

    # 构建词典
    dictionary = corpora.Dictionary(docs)
    dictionary.filter_extremes(
        no_below=FILTER_SETTINGS[lang]["NO_BELOW"],
        no_above=FILTER_SETTINGS[lang]["NO_ABOVE"],
        keep_n=FILTER_SETTINGS[lang]["KEEP_N"]
    )
    return dictionary, list(dictionary.doc2bow(doc) for doc in docs)

# 步骤6：模型评估（一致性和困惑度）
def calculate_metrics(model, corpus, docs, dictionary):
    """计算一致性和困惑度"""
    coherence = CoherenceModel(model, texts=docs, dictionary=dictionary, coherence='c_v').get_coherence()
    perplexity = model.log_perplexity(corpus)
    return coherence, perplexity

# 步骤7：主题强度随时间演变趋势图
def plot_topic_trends(df, model, corpus, dictionary, lang):
    """绘制主题强度随时间演变的趋势图"""
    df['year'] = df['date'].dt.year
    df['month'] = df['date'].dt.month
    df['topic_strength'] = 0

    topic_strength_by_year = []

    for year in range(2012, 2025):
        yearly_docs = df[df['year'] == year]
        topic_strength_year = []

        for _, doc in yearly_docs.iterrows():
            doc_bow = dictionary.doc2bow(doc['tokens'].split())
            topic_dist = model.get_document_topics(doc_bow, minimum_probability=0)
            topic_strength = {f"Topic_{tid + 1}": prob for tid, prob in topic_dist}
            topic_strength_year.append(topic_strength)

        # 计算每个主题的强度
        if topic_strength_year:
            df_topic_strength = pd.DataFrame(topic_strength_year)
            avg_topic_strength = df_topic_strength.mean() * 100
            topic_strength_by_year.append(avg_topic_strength)

    topic_strength_by_year = pd.DataFrame(topic_strength_by_year, index=range(2013, 2025))

    # 绘制主题强度随时间变化的趋势图
    plt.figure(figsize=(12, 8))
    for topic in topic_strength_by_year.columns:
        plt.plot(topic_strength_by_year.index, topic_strength_by_year[topic], label=topic)

    plt.xlabel('Year')
    plt.ylabel('Topic Strength (%)')
    plt.title(f"Topic Strength Over Time ({lang})")
    plt.legend()
    plt.grid(True)
    plt.show()  # 直接显示图形


# 步骤8：保存模型输出
def save_outputs(model, corpus, dictionary, lang, metrics, df):
    """保存所有输出结果"""
    save_path = os.path.join(OUTPUT_DIR, lang)

    # 确保保存路径存在
    os.makedirs(save_path, exist_ok=True)

    # 子步骤8.1：文档-主题矩阵
    doc_topic_list = []
    for doc_id, doc in enumerate(corpus):
        topic_dist = model.get_document_topics(doc, minimum_probability=0)
        doc_data = {"Document_ID": doc_id}
        doc_data.update({f"Topic_{tid + 1}": prob for tid, prob in topic_dist})
        doc_topic_list.append(doc_data)
    pd.DataFrame(doc_topic_list).to_excel(os.path.join(save_path, f"{lang}_document_topic_matrix.xlsx"), index=False)

    # 子步骤8.2：主题强度计算
    df_doc_topic = pd.DataFrame(doc_topic_list)
    topic_strength = df_doc_topic.drop("Document_ID", axis=1).mean() * 100
    pd.DataFrame({
        "Topic": topic_strength.index,
        "Strength(%)": topic_strength.values.round(2)
    }).to_excel(os.path.join(save_path, f"{lang}_topic_strength.xlsx"), index=False)

    # 子步骤8.3：主题词分布（30词）
    topics_data = []
    for topic_id in range(model.num_topics):
        words = model.show_topic(topic_id, topn=30)
        topics_data.append({
            "Topic": topic_id + 1,
            "Keywords": " | ".join([f"{w[0]}({w[1]:.3f})" for w in words])
        })
    pd.DataFrame(topics_data).to_excel(os.path.join(save_path, f"{lang}_topics.xlsx"), index=False)

    # 子步骤8.4：生成词云（语料库词云）
    all_words = sum([doc.split() for doc in df['tokens']], [])
    wordcloud = WordCloud(width=1200, height=800, background_color='white').generate(' '.join(all_words))
    plt.figure(figsize=(15, 10))
    plt.imshow(wordcloud)
    plt.axis("off")
    plt.title(f"Word Cloud for {lang}", fontsize=16)
    plt.show()  # 直接显示图形


    # 子步骤8.5：生成气泡图（pyLDAvis）
    vis = pyLDAvis.gensim_models.prepare(model, corpus, dictionary)
    pyLDAvis.save_html(vis, os.path.join(save_path, f"{lang}_bubble.html"))

    # 子步骤8.6：生成词关系网络图（pyvis）
    net = Network(notebook=True)
    for topic_id in range(model.num_topics):
        words = model.show_topic(topic_id, topn=30)
        for word, weight in words:
            net.add_node(word, label=word, title=f"Weight: {weight:.3f}")
    for topic_id in range(model.num_topics):
        words = model.show_topic(topic_id, topn=30)
        for i in range(len(words)):
            for j in range(i + 1, len(words)):
                net.add_edge(words[i][0], words[j][0], weight=0.5)  # 连接高权重的词

    net.save_graph(os.path.join(save_path, f"{lang}_word_network.html"))

    # 子步骤8.7：主题强度随时间变化的趋势图
    plot_topic_trends(df, model, corpus, dictionary, lang)

    # 子步骤8.8：评估曲线
    plt.figure(figsize=(15, 5))
    x = range(*PARAMS[lang]["TOPIC_RANGE"])
    plt.subplot(131)
    plt.plot(x, metrics['coherence'], 'b-o')
    plt.xlabel("Number of Topics")
    plt.ylabel("Coherence Score")
    plt.title(f"{lang} Topic Coherence")

    plt.subplot(132)
    plt.plot(x, metrics['perplexity'], 'r-^')
    plt.xlabel("Number of Topics")
    plt.ylabel("Perplexity")
    plt.title(f"{lang} Perplexity")

    plt.tight_layout()
    plt.show()  # 直接显示图形

    # 保存评估曲线图像
    plt.savefig(os.path.join(save_path, f"{lang}_metrics.png"))
    plt.close()


# 步骤9：LDA训练流程
def train_lda(docs, lang, df):
    """增强型训练流程"""
    dictionary, corpus = build_corpus(docs, lang)

    metrics = {'coherence': [], 'perplexity': []}
    topic_range = range(*PARAMS[lang]["TOPIC_RANGE"])

    # 训练多个主题模型
    for n_topics in tqdm(topic_range, desc="Training LDA Models"):
        print(f"Training LDA model with {n_topics} topics...")
        model = LdaMulticore(
            corpus=corpus,
            id2word=dictionary,
            num_topics=n_topics,
            passes=PARAMS[lang]["PASSES"],
            iterations=PARAMS[lang]["ITERATIONS"],
            workers=PARAMS[lang]["WORKERS"],
            alpha=PARAMS[lang]["DYNAMIC_ALPHA"],
            eta=PARAMS[lang]["DYNAMIC_ETA"],
            random_state=3407  # 固定随机种子
        )

        # 计算一致性和困惑度
        coherence, perplexity = calculate_metrics(model, corpus, docs, dictionary)
        metrics['coherence'].append(coherence)
        metrics['perplexity'].append(perplexity)

        # 打印一致性和困惑度
        print(f"Coherence: {coherence}, Perplexity: {perplexity}")

    # 自动选择最佳主题数：选择一致性得分最高的主题数
    best_n_topics = topic_range[np.argmax(metrics['coherence'])]
    print(f"\nBest number of topics based on coherence score: {best_n_topics}")

    # 输出一致性得分，并显示所有主题的得分
    print("Coherence scores for different topic numbers:")
    for n, score in zip(topic_range, metrics['coherence']):
        print(f"Topics: {n}, Coherence Score: {score:.4f}")

    # 绘制一致性得分和困惑度评估曲线，并保存为 PNG 文件
    plt.figure(figsize=(15, 5))

    # 一致性得分曲线
    plt.subplot(131)
    plt.plot(topic_range, metrics['coherence'], 'b-o')
    plt.xlabel("Number of Topics")
    plt.ylabel("Coherence Score")
    plt.title(f"{lang} Topic Coherence")

    # 困惑度曲线
    plt.subplot(132)
    plt.plot(topic_range, metrics['perplexity'], 'r-^')
    plt.xlabel("Number of Topics")
    plt.ylabel("Perplexity")
    plt.title(f"{lang} Perplexity")

    plt.tight_layout()

    # 保存评估曲线图像
    eval_plot_path = os.path.join(OUTPUT_DIR, f"{lang}_metrics.png")
    plt.savefig(eval_plot_path)
    plt.show()  # 显示图形

    # 训练并保存输出
    model = LdaMulticore(
        corpus=corpus,
        id2word=dictionary,
        num_topics=best_n_topics,
        passes=PARAMS[lang]["PASSES"],
        iterations=PARAMS[lang]["ITERATIONS"],
        workers=PARAMS[lang]["WORKERS"],
        alpha=PARAMS[lang]["DYNAMIC_ALPHA"],
        eta=PARAMS[lang]["DYNAMIC_ETA"]
    )
    save_outputs(model, corpus, dictionary, lang, metrics, df)


# 步骤10：主流程
def main():
    """执行流程"""
    lang = "USA"  # 语言选择
    docs, df = load_data(INPUT_PATH, lang)

    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # 训练LDA模型
    train_lda(docs, lang, df)

    print("LDA model training and evaluation complete!")

if __name__ == "__main__":
    main()