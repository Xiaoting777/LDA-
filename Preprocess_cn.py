import pandas as pd
import re
import nltk
import spacy
import logging
from nltk.corpus import stopwords

# ğŸ”¹ 1. ç¡®ä¿ NLTK èµ„æºå·²ä¸‹è½½
nltk.download('stopwords')
nltk.download('punkt')

# ğŸ”¹ 2. é…ç½®æ–‡ä»¶è·¯å¾„
input_path = "/Users/chenxiaoting/Downloads/æˆ‘çš„æ–‡ä»¶/åšå£«å­¦ä¹ /å¤§è®ºæ–‡/è®ºæ–‡æ•°æ®/txt 2025/china.csv"
output_tokens_path = "/Users/chenxiaoting/Downloads/æˆ‘çš„æ–‡ä»¶/åšå£«å­¦ä¹ /å¤§è®ºæ–‡/è®ºæ–‡æ•°æ®/txt 2025/Pre/china_cleaned_tokens_lemm.csv"
output_meta_path = "/Users/chenxiaoting/Downloads/æˆ‘çš„æ–‡ä»¶/åšå£«å­¦ä¹ /å¤§è®ºæ–‡/è®ºæ–‡æ•°æ®/txt 2025/Pre/china_metadata.csv"
extra_stopwords_path = "/Users/chenxiaoting/Downloads/æˆ‘çš„æ–‡ä»¶/åšå£«å­¦ä¹ /å¤§è®ºæ–‡/è®ºæ–‡æ•°æ®/txt 2025/stopwords-en.txt"

# ğŸ”¹ 3. è‡ªå®šä¹‰åœç”¨è¯ï¼ˆ**æ‰€æœ‰çš„è¯ä¼šè¿›è¡Œè¯æ€§è¿˜åŸ**ï¼‰
raw_custom_stopwords = set([
    'china', 'chinese', 'usa', 'america', 'russia', 'country', 'year',
    '2020', '2021', '2022', '2023', '2024', 'people', 'government',
    'population', 'percent', 'new', 'said', 'would', 'could', 'also', 'many',
    'including', 'however', 'say', 'make', 'number', 'need', 'high', 'item',
    'subject', 're', 'edu', 'use', 'million', 'billion', 'yuan','due',
    'according', 'get', 'right', 'leave', 'should', 'data', 'based','day','photo'
])

# åŠ è½½ SpaCy è¿›è¡Œè¯æ€§è¿˜åŸï¼ˆlemmatizationï¼‰
nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])

# ğŸ”¹ 4. è¯»å– `stopwords-en.txt` å¹¶è¿›è¡Œè¯æ€§è¿˜åŸ
try:
    with open(extra_stopwords_path, 'r', encoding='utf-8') as file:
        file_stopwords = set(line.strip() for line in file if line.strip())  # å»é™¤ç©ºæ ¼å’Œç©ºè¡Œ
        print("âœ… Stopwords file successfully loaded.")
except FileNotFoundError:
    file_stopwords = set()
    print("âš ï¸ Stopwords file not found! Using only NLTK and custom stopwords.")

# ğŸ”¹ 5. å¯¹æ‰€æœ‰åœç”¨è¯è¿›è¡Œè¯æ€§è¿˜åŸï¼ˆLemmatizationï¼‰
custom_stopwords = set(token.lemma_ for word in raw_custom_stopwords for token in nlp(word))
lemmatized_file_stopwords = set(token.lemma_ for word in file_stopwords for token in nlp(word))

# ğŸ”¹ 6. ç»“åˆ NLTKã€æ–‡ä»¶ã€è‡ªå®šä¹‰åœç”¨è¯ï¼ˆæœ€ç»ˆçš„ stopwords é›†åˆï¼‰
stop_words = set(stopwords.words('english')) | custom_stopwords | lemmatized_file_stopwords

print(f"ğŸ”¹ï¸ åœç”¨è¯æ€»æ•°ï¼š{len(stop_words)}")
print(f"ğŸ”¹ï¸ ç¤ºä¾‹åœç”¨è¯ï¼ˆå‰20ä¸ªï¼‰ï¼š{list(stop_words)[:20]}")

# ğŸ”¹ 7. æ–‡æœ¬æ¸…æ´—å’Œè¯æ€§è¿˜åŸ
def clean_and_lemmatize(text):
    if not isinstance(text, str):
        return ''  # ä¸æ˜¯å­—ç¬¦ä¸²ï¼Œè¿”å›ç©ºå€¼

    # æ¸…ç†æ–‡æœ¬ä¸­çš„ URLã€HTML æ ‡ç­¾ã€é‚®ç®±ç­‰
    text = re.sub(r'http\S+|www\S+', '', text)
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(r'\S*@\S*\s?', '', text)
    text = re.sub(r'[^a-zA-Z\s]', ' ', text)  # åªä¿ç•™è‹±æ–‡å­—æ¯
    text = re.sub(r'\d+', '', text)  # ç§»é™¤æ•°å­—
    text = text.lower()

    # è¯å½¢è¿˜åŸ + åœç”¨è¯è¿‡æ»¤
    doc = nlp(text)
    words = [token.lemma_ for token in doc if token.lemma_ not in stop_words and token.pos_ in ['NOUN', 'ADJ', 'VERB', 'ADV']]

    # è°ƒè¯•è¾“å‡º
    print(f"ğŸ”¹ï¸ åŸå§‹æ–‡æœ¬: {text[:100]}")  # è¾“å‡ºå‰100ä¸ªå­—ç¬¦
    print(f"ğŸ”¹ï¸ æ¸…æ´—åæ–‡æœ¬: {' '.join(words)[:100]}")  # è¾“å‡ºå¤„ç†åå‰100ä¸ªå­—ç¬¦

    return ' '.join(words)

# ğŸ”¹ 8. æ—¥å¿—è®°å½•
logging.basicConfig(filename='preprocess_en.log', level=logging.INFO)
logging.info("Processing started...")

# ğŸ”¹ 9. å¤„ç†è‹±è¯­ CSV æ–‡ä»¶
def process_english_csv(input_path):
    print(f"ğŸ”¹ å¼€å§‹å¤„ç†æ–‡ä»¶ï¼š{input_path}")
    df = pd.read_csv(input_path)

    # ç¡®ä¿ CSV åŒ…å«éœ€è¦çš„åˆ—
    required_columns = ['source', 'title', 'date', 'content']
    for col in required_columns:
        if col not in df.columns:
            df[col] = ''

    # åˆå¹¶æ ‡é¢˜å’Œæ–‡æœ¬å†…å®¹
    print("ğŸ”¹ æ­£åœ¨åˆå¹¶æ ‡é¢˜å’Œæ–‡æœ¬...")
    df['full_content'] = df['title'].astype(str) + ' ' + df['content'].astype(str)

    # è¿›è¡Œæ–‡æœ¬æ¸…æ´—å’Œè¯æ€§è¿˜åŸ
    print("ğŸ”¹ æ­£åœ¨æ¸…æ´—å’Œè¯æ€§è¿˜åŸ...")
    df['cleaned_text'] = df['full_content'].apply(clean_and_lemmatize)

    # ç”Ÿæˆ LDA æ•°æ®ï¼ˆåˆ†è¯æ–‡æœ¬ï¼‰
    lda_data = df[['cleaned_text']].rename(columns={'cleaned_text': 'tokens'})
    lda_data['doc_id'] = lda_data.index

    # ç”Ÿæˆå…ƒæ•°æ®
    meta_data = df[['source', 'date']].copy()
    meta_data['doc_id'] = meta_data.index

    # ä¿å­˜å¤„ç†åçš„ CSV æ–‡ä»¶
    lda_data.to_csv(output_tokens_path, index=False, encoding='utf-8-sig')  # UTF-8-SIG è§£å†³ Excel ä¹±ç é—®é¢˜
    meta_data.to_csv(output_meta_path, index=False, encoding='utf-8-sig')

    logging.info(f"Processing completed! Tokens saved to {output_tokens_path}")
    logging.info(f"Metadata saved to {output_meta_path}")
    print(f"ğŸ‰ æ¸…æ´—å®Œæˆï¼åˆ†è¯æ–‡æœ¬å·²ä¿å­˜åˆ°ï¼š{output_tokens_path}")
    print(f"ğŸ‘Œ å…ƒæ•°æ®å·²ä¿å­˜åˆ°ï¼š{output_meta_path}")

if __name__ == "__main__":
    process_english_csv(input_path)
