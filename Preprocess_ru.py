import pandas as pd
import re
import nltk
import logging
import pymorphy2  # å¤„ç†ä¿„è¯­è¯å½¢è¿˜åŸ
from nltk.corpus import stopwords

# ğŸ”¹ 1. ç¡®ä¿ NLTK èµ„æºå·²ä¸‹è½½
nltk.download('stopwords')
nltk.download('punkt')

# ğŸ”¹ 2. é…ç½®æ–‡ä»¶è·¯å¾„
input_path = "/Users/chenxiaoting/Downloads/æˆ‘çš„æ–‡ä»¶/åšå£«å­¦ä¹ /å¤§è®ºæ–‡/è®ºæ–‡æ•°æ®/txt 2025/RU.csv"
output_tokens_path = "/Users/chenxiaoting/Downloads/æˆ‘çš„æ–‡ä»¶/åšå£«å­¦ä¹ /å¤§è®ºæ–‡/è®ºæ–‡æ•°æ®/txt 2025/RU_cleaned_tokens_lemm.csv"
output_meta_path = "/Users/chenxiaoting/Downloads/æˆ‘çš„æ–‡ä»¶/åšå£«å­¦ä¹ /å¤§è®ºæ–‡/è®ºæ–‡æ•°æ®/txt 2025/RU_metadata.csv"
stopwords_russian_path = "/Users/chenxiaoting/Downloads/æˆ‘çš„æ–‡ä»¶/åšå£«å­¦ä¹ /å¤§è®ºæ–‡/è®ºæ–‡æ•°æ®/txt 2025/stopwords_russian.txt"

# ğŸ”¹ 3. è‡ªå®šä¹‰åœç”¨è¯ï¼ˆ**æ‰€æœ‰çš„è¯ä¼šè¿›è¡Œè¯å½¢è¿˜åŸ**ï¼‰
raw_custom_stopwords = set([
    'Ğ¼Ğ»Ğ½','Ñ‚Ñ‹ÑÑÑ‡Ğ°','Ğ¼Ğ»Ñ€Ğ´','Ñ€Ğ¾ÑÑĞ¸Ñ', 'ĞºĞ½Ñ€','ĞºĞ¸Ñ‚Ğ°Ğ¹', 'ÑÑ‚Ñ€Ğ°Ğ½Ğ°', 'Ğ³Ğ¾Ğ´', '2020', '2021', '2022', '2023', '2024',
    'Ğ»ÑĞ´Ğ¸', 'Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾', 'Ğ½Ğ°ÑĞµĞ»ĞµĞ½Ğ¸Ğµ','ÑĞ°Ğ¹Ñ‚','ÑĞ²Ğ»ÑÑ‚ÑŒÑÑ','Ğ¼ĞµÑÑÑ†','Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ','Ñ„Ğ¾Ñ‚Ğ¾','Ğ´ĞµĞºĞ°Ğ±Ñ€ÑŒ',
    'Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚','Ğ½Ğ¾Ğ²Ñ‹Ğ¹', 'ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ','Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚ÑŒÑÑ','ĞšĞ¸Ñ‚Ğ°Ğ¹Ñ†ĞµĞ²','Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚ÑŒ', 'Ğ¼Ğ¾Ğ³Ñƒ', 'ĞµÑ‰Ğµ', 'Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ',
    'Ğ¾Ğ´Ğ½Ğ°ĞºĞ¾', 'Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°', 'Ğ´ĞµĞ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ','Ñ‚Ğ°ÑÑ','Ñ€Ğ¸Ğ°', 'Ñ…Ğ¾Ñ‚ÑŒ', 'Ğ¼ĞµĞ¶Ğ´Ñƒ', 'Ğ²Ğ¿Ñ€Ğ¾Ñ‡ĞµĞ¼', 'ÑÑ‚Ğ¾Ñ‚', 'Ğ²ĞµÑÑŒ',
    'Ñ‚Ğ°ĞºĞ¾Ğ¹', 'ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹', 'ÑĞ²Ğ¾Ğ¹', 'Ğ½Ğ°Ñˆ', 'Ğ²Ğ°Ñˆ', 'ÑƒĞ¶Ğµ', 'ĞµÑ‰Ñ‘', 'Ğ»Ğ¸Ğ±Ğ¾','ÑĞ´ĞµĞ»Ğ°Ñ‚ÑŒ','Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ','Ğ´Ğ°Ñ‚ÑŒ',
    'ÑĞ½Ğ²Ğ°Ñ€ÑŒ','Ğ¸ÑĞ½ÑŒ','ÑĞ¾Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒ','ÑĞ¾Ğ¾Ğ±Ñ‰Ğ¸Ñ‚ÑŒ','ÑÑÑ‹Ğ»ĞºĞ°'
])

# åˆå§‹åŒ–è¯å½¢è¿˜åŸå™¨
morph = pymorphy2.MorphAnalyzer()

# ğŸ”¹ 4. è¯»å– `stopwords_russian.txt` å¹¶è¿›è¡Œè¯å½¢è¿˜åŸ
try:
    with open(stopwords_russian_path, 'r', encoding='utf-8') as file:
        file_stopwords = set(line.strip() for line in file if line.strip())  # å»é™¤ç©ºæ ¼å’Œç©ºè¡Œ
        print("âœ… Russian stopwords file successfully loaded.")
except FileNotFoundError:
    file_stopwords = set()
    print("âš ï¸ Stopwords file not found! Using only NLTK and custom stopwords.")

# ğŸ”¹ 5. å¯¹æ‰€æœ‰åœç”¨è¯è¿›è¡Œè¯æ€§è¿˜åŸï¼ˆLemmatizationï¼‰
custom_stopwords = set(morph.parse(word)[0].normal_form for word in raw_custom_stopwords)
lemmatized_file_stopwords = set(morph.parse(word)[0].normal_form for word in file_stopwords)

# ğŸ”¹ 6. ç»“åˆ NLTKã€æ–‡ä»¶ã€è‡ªå®šä¹‰åœç”¨è¯ï¼ˆæœ€ç»ˆçš„ stopwords é›†åˆï¼‰
stop_words = set(stopwords.words('russian')) | custom_stopwords | lemmatized_file_stopwords
print(f"ğŸ”¹ æ€»å…±åŠ è½½ {len(stop_words)} ä¸ªåœç”¨è¯ã€‚")

# ğŸ”¹ 7. é¢å¤–è§„åˆ™ï¼šéƒ¨åˆ†å‰ç¼€åŒ¹é…çš„åœç”¨è¯
def is_stopword(word):
    stopword_roots = ['ĞºĞ¸Ñ‚Ğ°Ğ¹', 'Ñ€Ğ¾ÑÑĞ¸']  # è¿‡æ»¤æ‰æ‰€æœ‰ä»¥è¿™äº›å‰ç¼€å¼€å¤´çš„è¯
    return any(word.startswith(root) for root in stopword_roots)

# ğŸ”¹ 8. æ–‡æœ¬æ¸…æ´—ä¸è¯æ€§è¿˜åŸ
def clean_and_lemmatize(text):
    if not isinstance(text, str):
        return ''  # ä¸æ˜¯å­—ç¬¦ä¸²ï¼Œè¿”å›ç©ºå€¼

    # æ¸…ç†æ–‡æœ¬ä¸­çš„ URLã€HTML æ ‡ç­¾ã€é‚®ç®±ç­‰
    text = re.sub(r'http\S+|www\S+', '', text)
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(r'\S*@\S*\s?', '', text)
    text = re.sub(r'[^\u0400-\u04FF\s]', ' ', text)  # åªä¿ç•™ä¿„æ–‡å­—æ¯
    text = re.sub(r'\d+', '', text)  # ç§»é™¤æ•°å­—
    text = text.lower()

    # è¯å½¢è¿˜åŸ + åœç”¨è¯è¿‡æ»¤
    words = [
        morph.parse(word)[0].normal_form
        for word in text.split()
        if morph.parse(word)[0].normal_form not in stop_words and not is_stopword(word)
    ]

    return ' '.join(words)

# ğŸ”¹ 9. æ—¥å¿—è®°å½•
logging.basicConfig(filename='preprocess_ru.log', level=logging.INFO)
logging.info("Processing started...")

# ğŸ”¹ 10. å¤„ç†ä¿„æ–‡ CSV æ–‡ä»¶
def process_russian_csv(input_path):
    print(f"ğŸ”¹ å¼€å§‹å¤„ç†æ–‡ä»¶ï¼š{input_path}")
    df = pd.read_csv(input_path, encoding='utf-8')  # æŒ‡å®šç¼–ç  UTF-8

    # ç¡®ä¿ CSV åŒ…å«éœ€è¦çš„åˆ—
    required_columns = ['source', 'title', 'date', 'content']
    for col in required_columns:
        if col not in df.columns:
            df[col] = ''

    # åˆå¹¶æ ‡é¢˜å’Œæ–‡æœ¬å†…å®¹
    print("ğŸ”¹ æ­£åœ¨åˆå¹¶æ ‡é¢˜å’Œæ–‡æœ¬...")
    df['full_content'] = df['title'].astype(str) + ' ' + df['content'].astype(str)

    # è¿›è¡Œæ–‡æœ¬æ¸…æ´—å’Œè¯æ€§è¿˜åŸ
    print("ğŸ”¹ æ­£åœ¨æ¸…æ´—å’Œè¯æ€§è¿˜åŸ...")
    df['cleaned_text'] = df['full_content'].apply(clean_and_lemmatize)

    # ç”Ÿæˆ LDA æ•°æ®ï¼ˆåˆ†è¯æ–‡æœ¬ï¼‰
    lda_data = df[['cleaned_text']].rename(columns={'cleaned_text': 'tokens'})
    lda_data['doc_id'] = lda_data.index

    # ç”Ÿæˆå…ƒæ•°æ®
    meta_data = df[['source', 'date']].copy()
    meta_data['doc_id'] = meta_data.index

    # ä¿å­˜å¤„ç†åçš„ CSV æ–‡ä»¶
    lda_data.to_csv(output_tokens_path, index=False, encoding='utf-8-sig')  # UTF-8-SIG è§£å†³ Excel ä¹±ç é—®é¢˜
    meta_data.to_csv(output_meta_path, index=False, encoding='utf-8-sig')

    logging.info(f"Processing completed! Tokens saved to {output_tokens_path}")
    logging.info(f"Metadata saved to {output_meta_path}")
    print(f"ğŸ‰ æ¸…æ´—å®Œæˆï¼åˆ†è¯æ–‡æœ¬å·²ä¿å­˜åˆ°ï¼š{output_tokens_path}")
    print(f"ğŸ‘Œ å…ƒæ•°æ®å·²ä¿å­˜åˆ°ï¼š{output_meta_path}")

if __name__ == "__main__":
    process_russian_csv(input_path)
