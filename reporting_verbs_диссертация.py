# -*- coding: utf-8 -*-
"""Reporting_verbs Ğ”Ğ¸ÑÑĞµÑ€Ñ‚Ğ°Ñ†Ğ¸Ñ.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JLDp-LZqf8mB-XiNfJFtdfxU7i0QOZJL

#CHINA+USA
import spacy
import pandas as pd

# ==================== åŠ è½½ NLP æ¨¡å‹ ====================
try:
    nlp = spacy.load("en_core_web_lg")  # å°è¯•åŠ è½½æ›´å¼ºçš„ `lg` æ¨¡å‹ï¼Œæé«˜ç²¾åº¦
    print("âœ… Loaded en_core_web_lg")
except:
    nlp = spacy.load("en_core_web_sm")  # å¦‚æœ `lg` æ¨¡å‹ä¸å¯ç”¨ï¼Œé™çº§ä½¿ç”¨ `sm` æ¨¡å‹
    print("âš ï¸ Falling back to en_core_web_sm")

# ==================== åŠ è½½è½¬è¿°åŠ¨è¯ ====================
VERB_CSV_PATH = "/content/yinyu/verbs.csv"  # å­˜å‚¨è½¬è¿°åŠ¨è¯çš„ CSV æ–‡ä»¶è·¯å¾„
OUTPUT_PATH = "/content/yinyu/result_CN.xlsx"  # ç»“æœè¾“å‡ºçš„ Excel æ–‡ä»¶è·¯å¾„
REPORT_PATH = "/content/yinyu/report_CN.xlsx"  # ç»Ÿè®¡åˆ†ææŠ¥å‘Š Excel æ–‡ä»¶è·¯å¾„

def load_reporting_verbs():
    '''ä» CSV æ–‡ä»¶åŠ è½½è½¬è¿°åŠ¨è¯å’Œå…¶å¯¹åº”çš„æƒ…æ„Ÿææ€§'''
    df = pd.read_csv(VERB_CSV_PATH)
    # åˆ›å»ºä¸€ä¸ªå­—å…¸ï¼Œé”®æ˜¯å°å†™çš„åŠ¨è¯ï¼Œå€¼æ˜¯ (åŸå§‹åŠ¨è¯, ææ€§)
    verbs = {row["token"].lower(): (row["token"], row["polarity"]) for _, row in df.iterrows()}
    return verbs

REPORTING_VERBS = load_reporting_verbs()

# ==================== é¢„å®šä¹‰è½¬è¿°çŸ­è¯­ ====================
REPORTING_PHRASES = {
    "according to", "as per", "based on", "in the view of",
    "as reported by", "from the perspective of", "by the account of","laid out arguments"
}

# ==================== é¢„å¤„ç†æ–‡æœ¬ ====================
def clean_and_split_text(text):
    '''æ¸…ç†è¾“å…¥æ–‡æœ¬å¹¶è¿›è¡Œåˆ†å¥å¤„ç†'''
    text = text.strip().replace("\n\n", ". ").replace("\n", " ").replace("  ", " ")
    doc = nlp(text)
    return [sent.text.strip() for sent in doc.sents if sent.text.strip()]

# ==================== ç›´æ¥å¼•è¯­æ£€æµ‹ ====================
def detect_direct_quotes(doc):
    '''æ£€æµ‹ç›´æ¥å¼•è¯­ï¼ˆå¸¦å¼•å·çš„éƒ¨åˆ†ï¼‰
    é€»è¾‘ï¼š
    1. è¯†åˆ«æ–‡æœ¬ä¸­çš„å¼•å· (`"`, `â€œ`, `â€`, `â€™`)ï¼Œæ‰¾åˆ°ç›´æ¥å¼•è¯­ç‰‡æ®µã€‚
    2. åœ¨å¼•å·åæˆ–å¼•å·å‰æœç´¢æ˜¯å¦å­˜åœ¨è½¬è¿°åŠ¨è¯ï¼ˆreporting verbï¼‰ã€‚
    3. è®°å½•æ‰¾åˆ°çš„è½¬è¿°åŠ¨è¯ï¼Œå¹¶æ ‡è®°ä¸ºç›´æ¥å¼•è¯­ï¼ˆdirectï¼‰ã€‚
    '''
    quote_list = []  # å­˜å‚¨æ£€æµ‹åˆ°çš„ç›´æ¥å¼•è¯­
    detected_direct_verbs = set()  # è®°å½•å·²ç»æ£€æµ‹å‡ºçš„è½¬è¿°åŠ¨è¯
    open_quote = None  # è¿½è¸ªæ˜¯å¦å¼€å¯å¼•å·

    for token in doc:
        if token.text in ['"', "â€œ", "â€", "'"] and open_quote is None:
            open_quote = token.i  # è®°å½•å¼•å·èµ·ç‚¹
        elif token.text in ['"', "â€œ", "â€", "'"] and open_quote is not None:
            if open_quote + 1 < token.i:
                verb, polarity = None, None

                # åœ¨å¼•å·åæŸ¥æ‰¾è½¬è¿°åŠ¨è¯
                for i in range(token.i + 1, len(doc)):
                    if doc[i].pos_ == "VERB" and doc[i].lemma_.lower() in REPORTING_VERBS:
                        verb, polarity = REPORTING_VERBS[doc[i].lemma_.lower()]
                        detected_direct_verbs.add(verb)
                        break

                # åœ¨å¼•å·å‰æŸ¥æ‰¾è½¬è¿°åŠ¨è¯ï¼ˆè‹¥åé¢æœªæ‰¾åˆ°ï¼‰
                if verb is None:
                    for i in range(open_quote - 1, -1, -1):
                        if doc[i].pos_ == "VERB" and doc[i].lemma_.lower() in REPORTING_VERBS:
                            verb, polarity = REPORTING_VERBS[doc[i].lemma_.lower()]
                            detected_direct_verbs.add(verb)
                            break

                # è®°å½•æœ‰æ•ˆçš„è½¬è¿°åŠ¨è¯
                if verb:
                    quote_list.append({
                        "verb": verb,
                        "polarity": polarity,
                        "quote_type": "direct"
                    })
            open_quote = None  # å…³é—­å¼•å·
    return quote_list, detected_direct_verbs

# ==================== é—´æ¥å¼•è¯­æ£€æµ‹ ====================
def detect_indirect_quotes(doc, detected_direct_verbs):

    '''æ£€æµ‹é—´æ¥å¼•è¯­ï¼ˆä¸å«å¼•å·çš„éƒ¨åˆ†ï¼‰
    é€»è¾‘ï¼š
    1. è¯†åˆ«ä¾å­˜å…³ç³»ä¸º `ccomp`ã€`xcomp` æˆ– `attr`ï¼Œè¿™äº›é€šå¸¸è¡¨æ˜ä»å¥ä¾èµ–äºè½¬è¿°åŠ¨è¯ã€‚
    2. è‹¥è¯¥åŠ¨è¯å°šæœªåœ¨ç›´æ¥å¼•è¯­ä¸­è¢«è®°å½•ï¼Œåˆ™æ ‡è®°ä¸ºé—´æ¥å¼•è¯­ï¼ˆindirectï¼‰ã€‚
    3. é¢å¤–æ£€æŸ¥æ˜¯å¦åŒ…å«é¢„å®šä¹‰çš„è½¬è¿°çŸ­è¯­ï¼ˆå¦‚ `according to`ï¼‰ã€‚
    '''

    quote_list = set()
    for word in doc:
        if word.dep_ in ("ccomp", "xcomp") or (word.dep_ == "attr" and word.head.pos_ == "VERB"):
            lemma = word.head.lemma_.lower()
            if lemma in REPORTING_VERBS and word.head.text not in detected_direct_verbs:
                verb, polarity = REPORTING_VERBS[lemma]
                quote_list.add((verb, polarity, "indirect"))
        if word.dep_ == "prep":
            phrase = f"{word.text.lower()} {doc[word.i + 1].text.lower()}" if word.i + 1 < len(doc) else word.text.lower()
            if phrase in REPORTING_PHRASES:
                quote_list.add((phrase, "neutral", "indirect"))
    return [{"verb": v, "polarity": p, "quote_type": t} for v, p, t in quote_list]

# ==================== å¤„ç†æ–‡ä»¶ ====================
def process_file(input_path):
    df = pd.read_csv(input_path)
    all_data = []

    for text in df["token"]:
        # æ£€æŸ¥ text æ˜¯å¦ä¸ºå­—ç¬¦ä¸²ï¼Œç„¶åå†è¿›è¡Œå¤„ç†
        if isinstance(text, str):
            sentences = clean_and_split_text(text)
            for sent in sentences:
                doc = nlp(sent)
                direct_quotes, detected_direct_verbs = detect_direct_quotes(doc)
                indirect_quotes = [] if direct_quotes else detect_indirect_quotes(doc, detected_direct_verbs)
                for res in direct_quotes + indirect_quotes:
                    all_data.append({"sentence": sent, "verb": res["verb"], "polarity": res["polarity"], "quote_type": res["quote_type"]})
        else:
            # å¤„ç†éå­—ç¬¦ä¸²å€¼ï¼Œä¾‹å¦‚ï¼Œæ‰“å°è­¦å‘Šæˆ–è·³è¿‡
            print(f"è­¦å‘Šï¼š'token' åˆ—ä¸­è·³è¿‡éå­—ç¬¦ä¸²å€¼ï¼š{text}")

    # å»é‡å¤„ç†
    df_results = pd.DataFrame(all_data).drop_duplicates()
    df_results.to_excel(OUTPUT_PATH, index=False)
    print(f"ğŸ‰ ç»“æœå·²ä¿å­˜: {OUTPUT_PATH}")

    # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    generate_report(df_results)

def generate_report(df_results):
    '''ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š'''
    verb_counts = df_results.groupby(["verb", "polarity"]).size().reset_index(name="count").sort_values(by="count", ascending=False)
    polarity_counts = df_results["polarity"].value_counts().reset_index()
    polarity_counts.columns = ["polarity", "count"]
    polarity_counts["percentage"] = polarity_counts["count"] / polarity_counts["count"].sum() * 100
    quote_type_counts = df_results["quote_type"].value_counts().reset_index()
    quote_type_counts.columns = ["quote_type", "count"]
    quote_type_counts["percentage"] = quote_type_counts["count"] / quote_type_counts["count"].sum() * 100

    with pd.ExcelWriter(REPORT_PATH) as writer:
        verb_counts.to_excel(writer, sheet_name="Verb Counts", index=False)
        polarity_counts.to_excel(writer, sheet_name="Polarity Distribution", index=False)
        quote_type_counts.to_excel(writer, sheet_name="Quote Type Distribution", index=False)
    print(f"ğŸ“Š ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜: {REPORT_PATH}")

# è¿è¡Œ
INPUT_PATH = "/content/yinyu/china_yuanwen.csv"
process_file(INPUT_PATH)
"""

#CHINA+USA
import spacy
import pandas as pd

# ==================== åŠ è½½ NLP æ¨¡å‹ ====================
try:
    nlp = spacy.load("en_core_web_lg")  # å°è¯•åŠ è½½æ›´å¼ºçš„ `lg` æ¨¡å‹ï¼Œæé«˜ç²¾åº¦
    print("âœ… Loaded en_core_web_lg")
except:
    nlp = spacy.load("en_core_web_sm")  # å¦‚æœ `lg` æ¨¡å‹ä¸å¯ç”¨ï¼Œé™çº§ä½¿ç”¨ `sm` æ¨¡å‹
    print("âš ï¸ Falling back to en_core_web_sm")

# ==================== åŠ è½½è½¬è¿°åŠ¨è¯ ====================
VERB_CSV_PATH = "/content/yinyu/verbs.csv"  # å­˜å‚¨è½¬è¿°åŠ¨è¯çš„ CSV æ–‡ä»¶è·¯å¾„
OUTPUT_PATH = "/content/yinyu/result_CN.xlsx"  # ç»“æœè¾“å‡ºçš„ Excel æ–‡ä»¶è·¯å¾„
REPORT_PATH = "/content/yinyu/report_CN.xlsx"  # ç»Ÿè®¡åˆ†ææŠ¥å‘Š Excel æ–‡ä»¶è·¯å¾„

def load_reporting_verbs():
    """ä» CSV æ–‡ä»¶åŠ è½½è½¬è¿°åŠ¨è¯å’Œå…¶å¯¹åº”çš„æƒ…æ„Ÿææ€§"""
    df = pd.read_csv(VERB_CSV_PATH)
    # åˆ›å»ºä¸€ä¸ªå­—å…¸ï¼Œé”®æ˜¯å°å†™çš„åŠ¨è¯ï¼Œå€¼æ˜¯ (åŸå§‹åŠ¨è¯, ææ€§)
    verbs = {row["token"].lower(): (row["token"], row["polarity"]) for _, row in df.iterrows()}
    return verbs

REPORTING_VERBS = load_reporting_verbs()

# ==================== é¢„å®šä¹‰è½¬è¿°çŸ­è¯­ ====================
REPORTING_PHRASES = {
    "according to", "as per", "based on", "in the view of",
    "as reported by", "from the perspective of", "by the account of","laid out arguments"
}

# ==================== é¢„å¤„ç†æ–‡æœ¬ ====================
def clean_and_split_text(text):
    """æ¸…ç†è¾“å…¥æ–‡æœ¬å¹¶è¿›è¡Œåˆ†å¥å¤„ç†"""
    text = text.strip().replace("\n\n", ". ").replace("\n", " ").replace("  ", " ")
    doc = nlp(text)
    return [sent.text.strip() for sent in doc.sents if sent.text.strip()]

# ==================== ç›´æ¥å¼•è¯­æ£€æµ‹ ====================
def detect_direct_quotes(doc):
    """æ£€æµ‹ç›´æ¥å¼•è¯­ï¼ˆå¸¦å¼•å·çš„éƒ¨åˆ†ï¼‰
    é€»è¾‘ï¼š
    1. è¯†åˆ«æ–‡æœ¬ä¸­çš„å¼•å· (`"`, `â€œ`, `â€`, `â€™`)ï¼Œæ‰¾åˆ°ç›´æ¥å¼•è¯­ç‰‡æ®µã€‚
    2. åœ¨å¼•å·åæˆ–å¼•å·å‰æœç´¢æ˜¯å¦å­˜åœ¨è½¬è¿°åŠ¨è¯ï¼ˆreporting verbï¼‰ã€‚
    3. è®°å½•æ‰¾åˆ°çš„è½¬è¿°åŠ¨è¯ï¼Œå¹¶æ ‡è®°ä¸ºç›´æ¥å¼•è¯­ï¼ˆdirectï¼‰ã€‚
    """
    quote_list = []  # å­˜å‚¨æ£€æµ‹åˆ°çš„ç›´æ¥å¼•è¯­
    detected_direct_verbs = set()  # è®°å½•å·²ç»æ£€æµ‹å‡ºçš„è½¬è¿°åŠ¨è¯
    open_quote = None  # è¿½è¸ªæ˜¯å¦å¼€å¯å¼•å·

    for token in doc:
        if token.text in ['"', "â€œ", "â€", "'"] and open_quote is None:
            open_quote = token.i  # è®°å½•å¼•å·èµ·ç‚¹
        elif token.text in ['"', "â€œ", "â€", "'"] and open_quote is not None:
            if open_quote + 1 < token.i:
                verb, polarity = None, None

                # åœ¨å¼•å·åæŸ¥æ‰¾è½¬è¿°åŠ¨è¯
                for i in range(token.i + 1, len(doc)):
                    if doc[i].pos_ == "VERB" and doc[i].lemma_.lower() in REPORTING_VERBS:
                        verb, polarity = REPORTING_VERBS[doc[i].lemma_.lower()]
                        detected_direct_verbs.add(verb)
                        break

                # åœ¨å¼•å·å‰æŸ¥æ‰¾è½¬è¿°åŠ¨è¯ï¼ˆè‹¥åé¢æœªæ‰¾åˆ°ï¼‰
                if verb is None:
                    for i in range(open_quote - 1, -1, -1):
                        if doc[i].pos_ == "VERB" and doc[i].lemma_.lower() in REPORTING_VERBS:
                            verb, polarity = REPORTING_VERBS[doc[i].lemma_.lower()]
                            detected_direct_verbs.add(verb)
                            break

                # è®°å½•æœ‰æ•ˆçš„è½¬è¿°åŠ¨è¯
                if verb:
                    quote_list.append({
                        "verb": verb,
                        "polarity": polarity,
                        "quote_type": "direct"
                    })
            open_quote = None  # å…³é—­å¼•å·
    return quote_list, detected_direct_verbs

# ==================== é—´æ¥å¼•è¯­æ£€æµ‹ ====================
def detect_indirect_quotes(doc, detected_direct_verbs):

    """æ£€æµ‹é—´æ¥å¼•è¯­ï¼ˆä¸å«å¼•å·çš„éƒ¨åˆ†ï¼‰
    é€»è¾‘ï¼š
    1. è¯†åˆ«ä¾å­˜å…³ç³»ä¸º `ccomp`ã€`xcomp` æˆ– `attr`ï¼Œè¿™äº›é€šå¸¸è¡¨æ˜ä»å¥ä¾èµ–äºè½¬è¿°åŠ¨è¯ã€‚
    2. è‹¥è¯¥åŠ¨è¯å°šæœªåœ¨ç›´æ¥å¼•è¯­ä¸­è¢«è®°å½•ï¼Œåˆ™æ ‡è®°ä¸ºé—´æ¥å¼•è¯­ï¼ˆindirectï¼‰ã€‚
    3. é¢å¤–æ£€æŸ¥æ˜¯å¦åŒ…å«é¢„å®šä¹‰çš„è½¬è¿°çŸ­è¯­ï¼ˆå¦‚ `according to`ï¼‰ã€‚
    """

    quote_list = set()
    for word in doc:
        if word.dep_ in ("ccomp", "xcomp") or (word.dep_ == "attr" and word.head.pos_ == "VERB"):
            lemma = word.head.lemma_.lower()
            if lemma in REPORTING_VERBS and word.head.text not in detected_direct_verbs:
                verb, polarity = REPORTING_VERBS[lemma]
                quote_list.add((verb, polarity, "indirect"))
        if word.dep_ == "prep":
            phrase = f"{word.text.lower()} {doc[word.i + 1].text.lower()}" if word.i + 1 < len(doc) else word.text.lower()
            if phrase in REPORTING_PHRASES:
                quote_list.add((phrase, "neutral", "indirect"))
    return [{"verb": v, "polarity": p, "quote_type": t} for v, p, t in quote_list]

# ==================== å¤„ç†æ–‡ä»¶ ====================
def process_file(input_path):
    df = pd.read_csv(input_path)
    all_data = []

    for text in df["token"]:
        # æ£€æŸ¥ text æ˜¯å¦ä¸ºå­—ç¬¦ä¸²ï¼Œç„¶åå†è¿›è¡Œå¤„ç†
        if isinstance(text, str):
            sentences = clean_and_split_text(text)
            for sent in sentences:
                doc = nlp(sent)
                direct_quotes, detected_direct_verbs = detect_direct_quotes(doc)
                indirect_quotes = [] if direct_quotes else detect_indirect_quotes(doc, detected_direct_verbs)
                for res in direct_quotes + indirect_quotes:
                    all_data.append({"sentence": sent, "verb": res["verb"], "polarity": res["polarity"], "quote_type": res["quote_type"]})
        else:
            # å¤„ç†éå­—ç¬¦ä¸²å€¼ï¼Œä¾‹å¦‚ï¼Œæ‰“å°è­¦å‘Šæˆ–è·³è¿‡
            print(f"è­¦å‘Šï¼š'token' åˆ—ä¸­è·³è¿‡éå­—ç¬¦ä¸²å€¼ï¼š{text}")

    # å»é‡å¤„ç†
    df_results = pd.DataFrame(all_data).drop_duplicates()
    df_results.to_excel(OUTPUT_PATH, index=False)
    print(f"ğŸ‰ ç»“æœå·²ä¿å­˜: {OUTPUT_PATH}")

    # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    generate_report(df_results)

def generate_report(df_results):
    """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
    verb_counts = df_results.groupby(["verb", "polarity"]).size().reset_index(name="count").sort_values(by="count", ascending=False)
    polarity_counts = df_results["polarity"].value_counts().reset_index()
    polarity_counts.columns = ["polarity", "count"]
    polarity_counts["percentage"] = polarity_counts["count"] / polarity_counts["count"].sum() * 100
    quote_type_counts = df_results["quote_type"].value_counts().reset_index()
    quote_type_counts.columns = ["quote_type", "count"]
    quote_type_counts["percentage"] = quote_type_counts["count"] / quote_type_counts["count"].sum() * 100

    with pd.ExcelWriter(REPORT_PATH) as writer:
        verb_counts.to_excel(writer, sheet_name="Verb Counts", index=False)
        polarity_counts.to_excel(writer, sheet_name="Polarity Distribution", index=False)
        quote_type_counts.to_excel(writer, sheet_name="Quote Type Distribution", index=False)
    print(f"ğŸ“Š ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜: {REPORT_PATH}")

# è¿è¡Œ
INPUT_PATH = "/content/yinyu/china_yuanwen.csv"
process_file(INPUT_PATH)

import re
import pandas as pd
import pymorphy2
from tqdm import tqdm

# åˆå§‹åŒ– pymorphy2 è¯å½¢è¿˜åŸå™¨
morph = pymorphy2.MorphAnalyzer()

# ==================== åŠ è½½è½¬è¿°åŠ¨è¯ ====================
VERB_CSV_PATH = "/Users/chenxiaoting/Downloads/æˆ‘çš„æ–‡ä»¶/åšå£«å­¦ä¹ /å¤§è®ºæ–‡/è®ºæ–‡æ•°æ®/yinyu/verbs_ru.csv"  # åŠ¨è¯è¯åº“è·¯å¾„ï¼ˆåŒ…å«åŸå½¢ï¼‰


def load_reporting_verbs():
    """ä» CSV æ–‡ä»¶åŠ è½½è½¬è¿°åŠ¨è¯å’Œå…¶å¯¹åº”çš„æƒ…æ„Ÿææ€§ï¼ˆåŸå½¢ï¼‰"""
    df = pd.read_csv(VERB_CSV_PATH)
    verbs = {row["token"].lower(): (row["token"], row["polarity"]) for _, row in df.iterrows()}
    return verbs


REPORTING_VERBS = load_reporting_verbs()

# ==================== é¢„å®šä¹‰è½¬è¿°çŸ­è¯­ ====================
REPORTING_PHRASES = {
    "Ğ¿Ğ¾ ÑĞ»Ğ¾Ğ²Ğ°Ğ¼", "Ğ¿Ğ¾ ĞµĞ³Ğ¾ ÑĞ»Ğ¾Ğ²Ğ°Ğ¼", "Ğ¿Ğ¾ ĞµĞµ ÑĞ»Ğ¾Ğ²Ğ°Ğ¼", "Ğ¿Ğ¾ Ğ¸Ñ… ÑĞ»Ğ¾Ğ²Ğ°Ğ¼",
    "Ğ¿Ğ¾ Ğ¼Ğ½ĞµĞ½Ğ¸Ñ", "Ğ¿Ğ¾ ĞµĞ³Ğ¾ Ğ¼Ğ½ĞµĞ½Ğ¸Ñ", "Ğ¿Ğ¾ ĞµĞµ Ğ¼Ğ½ĞµĞ½Ğ¸Ñ", "Ğ¿Ğ¾ Ğ¸Ñ… Ğ¼Ğ½ĞµĞ½Ğ¸Ñ",
    "Ğ¿Ğ¾ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼", "Ğ¿Ğ¾ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸", "Ğ¿Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼", "Ğ¿Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸ÑĞ¼",
    "Ğ¿Ğ¾ ÑĞ²ĞµĞ´ĞµĞ½Ğ¸ÑĞ¼", "Ğ¿Ğ¾ Ğ²ĞµÑ€ÑĞ¸Ğ¸", "Ğ¿Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸",
    "Ğ¿Ğ¾ Ğ½ĞµĞ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼", "Ğ¿Ğ¾ ÑĞ»ÑƒÑ…Ğ°Ğ¼", "Ğ¿Ğ¾ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ", "Ğ¿Ğ¾ Ğ¾Ñ†ĞµĞ½ĞºĞµ",
    "ÑĞ¾ ÑÑÑ‹Ğ»ĞºĞ¾Ğ¹ Ğ½Ğ°", "ĞºĞ°Ğº Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°Ñ‚ÑŒ", "ĞºĞ°Ğº Ğ·Ğ°ÑĞ²Ğ»ÑÑ‚ÑŒ", "ĞºĞ°Ğº Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ",
    "ĞºĞ°Ğº ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°Ñ‚ÑŒ", "ĞºĞ°Ğº Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ½ÑƒÑ‚ÑŒ", "ĞºĞ°Ğº Ğ¿Ğ¾ÑÑĞ½ÑÑ‚ÑŒ", "ĞºĞ°Ğº ÑĞ¾Ğ¾Ğ±Ñ‰Ğ¸Ñ‚ÑŒ",
    "ĞºĞ°Ğº Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ´Ğ¸Ñ‚ÑŒ", "ĞºĞ°Ğº Ğ¾Ğ±ÑŠÑĞ²Ğ¸Ñ‚ÑŒ", "ĞºĞ°Ğº Ğ·Ğ°Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ", "ĞºĞ°Ğº Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚ĞµÑ€ĞµÑ‡ÑŒ",
    "ĞºĞ°Ğº Ğ¾ÑÑƒĞ´Ğ¸Ñ‚ÑŒ", "ĞºĞ°Ğº Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ³Ğ½ÑƒÑ‚ÑŒ", "ĞºĞ°Ğº Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²",
    "ĞºĞ°Ğº Ñ€Ğ°ÑĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ²Ğ°Ñ‚ÑŒ", "ĞºĞ°Ğº Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ñ‚ÑŒÑÑ", "ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ",
    "ĞºĞ°Ğº Ğ¿Ñ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ´Ğ¸Ñ‚ÑŒ", "ĞºĞ°Ğº ÑƒĞ³Ñ€Ğ¾Ğ¶Ğ°Ñ‚ÑŒ", "ĞºĞ°Ğº Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ", "ĞºĞ°Ğº ÑĞ¿Ğ¾Ñ€Ğ¸Ñ‚ÑŒ",
    "ĞºĞ°Ğº Ğ´Ğ¸ÑĞºÑƒÑ‚Ğ¸Ñ€ÑƒĞµÑ‚", "ĞºĞ°Ğº Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚", "ĞºĞ°Ğº Ğ²Ğ¾Ğ·Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚", "ĞºĞ°Ğº Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ³Ğ°ĞµÑ‚",
    "ĞºĞ°Ğº Ñ€Ğ°ÑĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ²Ğ°Ğ»", "ĞºĞ°Ğº Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ÑÑ", "ĞºĞ°Ğº Ğ½Ğ°ÑÑ‚Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚",
    "ÑÑ…Ğ¾Ğ´ÑÑ‚ÑÑ Ğ²Ğ¾ Ğ¼Ğ½ĞµĞ½Ğ¸Ğ¸", "Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ", "Ğ¾Ğ±Ñ€Ğ°Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ"
}


# ==================== åŠ¨è¯è¯å½¢è¿˜åŸ ====================
def lemmatize_verb(verb):
    """ä½¿ç”¨ pymorphy2 è¿›è¡Œè¯å½¢è¿˜åŸ"""
    parsed = morph.parse(verb)[0]
    return parsed.normal_form  # è¿”å›è¯å½¢è¿˜åŸåçš„åŠ¨è¯åŸå½¢


def get_base_verb(verb):
    """è·å–åŠ¨è¯çš„åŸå½¢å’Œææ€§"""
    lemma = lemmatize_verb(verb.lower())
    if lemma in REPORTING_VERBS:
        return lemma, REPORTING_VERBS[lemma][1]  # è¿”å›åŸå½¢å’Œææ€§
    return verb, "æœªçŸ¥"  # æ— æ³•è¿˜åŸæ—¶è¿”å›åŸè¯


# ==================== ç›´æ¥å¼•è¯­æ£€æµ‹ï¼ˆæ­£åˆ™ï¼‰ ====================
DIRECT_SPEECH_PATTERNS = [
    r'["Â«â€](.*?)["Â»â€œ]',  # å¼•å·å†…çš„æ–‡æœ¬
    r'â€”\s+(.*)',  # ç ´æŠ˜å·å¼•å‡ºçš„æ–‡æœ¬
    r'(\w+):\s+["Â«â€](.*?)["Â»â€œ]'  # å†’å· + å¼•å·
]


def detect_direct_quotes(text):
    """æ£€æµ‹ç›´æ¥å¼•è¯­"""
    direct_quotes = []
    detected_direct_verbs = set()

    for pattern in DIRECT_SPEECH_PATTERNS:
        matches = re.findall(pattern, text)
        if matches:
            for match in matches:
                # æå–å¼•è¯­å†…å®¹å¹¶æ£€æŸ¥è½¬è¿°åŠ¨è¯
                quote = match[0] if isinstance(match, tuple) else match
                for token in re.findall(r'\w+', text):
                    lemma, polarity = get_base_verb(token)
                    if lemma in REPORTING_VERBS:
                        detected_direct_verbs.add(lemma)
                        direct_quotes.append({
                            "sentence": text,
                            "verb": token,
                            "lemma": lemma,
                            "polarity": polarity,
                            "quote_type": "direct"
                        })
    return direct_quotes, detected_direct_verbs


# ==================== é—´æ¥å¼•è¯­æ£€æµ‹ ====================
def detect_indirect_quotes(text, detected_direct_verbs):
    """æ£€æµ‹é—´æ¥å¼•è¯­ï¼ˆè¦æ±‚åŒ…å« 'Ñ‡Ñ‚Ğ¾' å’Œè½¬è¿°åŠ¨è¯/çŸ­è¯­ï¼‰"""
    indirect_quotes = []
    text_lower = text.lower()

    if 'Ñ‡Ñ‚Ğ¾' in text_lower:
        # æ£€æŸ¥è½¬è¿°åŠ¨è¯
        for token in re.findall(r'\w+', text):
            lemma, polarity = get_base_verb(token)
            if lemma in REPORTING_VERBS and lemma not in detected_direct_verbs:
                indirect_quotes.append({
                    "sentence": text,
                    "verb": token,
                    "lemma": lemma,
                    "polarity": polarity,
                    "quote_type": "indirect"
                })
                break
        # æ£€æŸ¥è½¬è¿°çŸ­è¯­
        for phrase in REPORTING_PHRASES:
            if phrase in text_lower:
                indirect_quotes.append({
                    "sentence": text,
                    "verb": phrase,
                    "lemma": phrase,
                    "polarity": "ä¸­æ€§",  # çŸ­è¯­çš„ææ€§éœ€å•ç‹¬å®šä¹‰ï¼ˆæ­¤å¤„ç®€åŒ–ï¼‰
                    "quote_type": "indirect"
                })
                break
    return indirect_quotes


# ==================== åˆ†å¥å¤„ç† ====================
def split_into_sentences(text):
    """å°†æ–‡æœ¬åˆ†å‰²æˆå¥å­"""
    return re.split(r'(?<=[.!?])\s+', text)


# ==================== ç»Ÿè®¡æŠ¥å‘Šç”Ÿæˆ ====================
def generate_report(df_results):
    """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
    verb_counts = df_results.groupby(["verb", "polarity"]).size().reset_index(name="count").sort_values(by="count",
                                                                                                        ascending=False)
    # ç»Ÿè®¡è½¬è¿°çŸ­è¯­
    phrase_counts = df_results[df_results['quote_type'] == 'indirect'].groupby(["verb", "polarity"]).size().reset_index(
        name="count").sort_values(by="count", ascending=False)

    # è¾“å‡ºåŠ¨è¯å’ŒçŸ­è¯­ç»Ÿè®¡
    print(f"Verb counts: {verb_counts.shape[0]}")  # æ‰“å°åŠ¨è¯ç»Ÿè®¡æ¡æ•°
    print(f"Phrase counts: {phrase_counts.shape[0]}")  # æ‰“å°çŸ­è¯­ç»Ÿè®¡æ¡æ•°

    polarity_counts = df_results["polarity"].value_counts().reset_index()
    polarity_counts.columns = ["polarity", "count"]
    polarity_counts["percentage"] = polarity_counts["count"] / polarity_counts["count"].sum() * 100
    quote_type_counts = df_results["quote_type"].value_counts().reset_index()
    quote_type_counts.columns = ["quote_type", "count"]
    quote_type_counts["percentage"] = quote_type_counts["count"] / quote_type_counts["count"].sum() * 100

    with pd.ExcelWriter("/Users/chenxiaoting/Downloads/æˆ‘çš„æ–‡ä»¶/åšå£«å­¦ä¹ /å¤§è®ºæ–‡/è®ºæ–‡æ•°æ®/yinyu/report_RU.xlsx") as writer:
        verb_counts.to_excel(writer, sheet_name="Verb Counts", index=False)
        phrase_counts.to_excel(writer, sheet_name="Phrase Counts", index=False)  # æ·»åŠ çŸ­è¯­ç»Ÿè®¡
        polarity_counts.to_excel(writer, sheet_name="Polarity Distribution", index=False)
        quote_type_counts.to_excel(writer, sheet_name="Quote Type Distribution", index=False)

    print(f"ğŸ“Š ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜ report_RU.xlsx")


# ==================== ä¸»å¤„ç†æµç¨‹ ====================
def process_file(input_path):
    df = pd.read_csv(input_path)
    all_data = []

    for text in tqdm(df["token"], desc="Processing", unit="text"):
        if isinstance(text, str):
            sentences = split_into_sentences(text)
            for sentence in sentences:
                # ç›´æ¥å¼•è¯­æ£€æµ‹
                direct_quotes, detected_verbs = detect_direct_quotes(sentence)
                # é—´æ¥å¼•è¯­æ£€æµ‹ï¼ˆä»…å½“æ— ç›´æ¥å¼•è¯­æ—¶ï¼‰
                indirect_quotes = detect_indirect_quotes(sentence, detected_verbs) if not direct_quotes else []
                all_data.extend(direct_quotes + indirect_quotes)

    # ä¿å­˜ç»“æœï¼ˆå»é™¤é‡å¤é¡¹ï¼‰
    df_results = pd.DataFrame(all_data).drop_duplicates()
    df_results.to_excel("/Users/chenxiaoting/Downloads/æˆ‘çš„æ–‡ä»¶/åšå£«å­¦ä¹ /å¤§è®ºæ–‡/è®ºæ–‡æ•°æ®/yinyu/result_RU.xlsx", index=False)
    print("ğŸ‰ ç»“æœå·²ä¿å­˜ result_RU.xlsx")


# ğŸš€ è¿è¡Œ
process_file("/Users/chenxiaoting/Downloads/æˆ‘çš„æ–‡ä»¶/åšå£«å­¦ä¹ /å¤§è®ºæ–‡/è®ºæ–‡æ•°æ®/yinyu/file_yuan/RU_yuanwen.csv")

#æ‰¾å‡ºæ¯è¡Œä¸­å¼•å·å†…å­—ç¬¦æ•°å°äº3çš„å¼•ç”¨å†…å®¹
import re
import pandas as pd

# è¯»å–æ–‡ä»¶
file_path = "/content/result_RU.xlsx"
df = pd.read_excel(file_path, usecols=["sentence"])  # åªè¯»å–ç¬¬ä¸€åˆ— "sentence"

# æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…åŒå¼•å·ï¼ˆåŒ…æ‹¬ä¿„è¯­åŒå¼•å·ï¼‰å†…çš„å†…å®¹
QUOTE_PATTERN = r'["]([^"]*?)["]'  # å…¼å®¹ä¿„è¯­åŒå¼•å·

def clean_text(text):
    """å»é™¤æ ‡ç‚¹ç¬¦å·ï¼Œé˜²æ­¢å½±å“å•è¯è®¡æ•°"""
    return re.sub(r'[^\w\s]', '', text)

def extract_short_word_quoted_text(text):
    """æŸ¥æ‰¾æ‰€æœ‰å¼•å·å†…å•è¯æ•°å°äºç­‰äº 3 çš„æ–‡æœ¬"""
    matches = re.findall(QUOTE_PATTERN, text)  # æŸ¥æ‰¾æ‰€æœ‰å¼•å·å†…çš„å†…å®¹
    short_words = [match for match in matches if len(clean_text(match).split()) <= 3]  # è¿‡æ»¤å•è¯æ•° â‰¤ 3
    return ", ".join(short_words) if short_words else ""

# å¤„ç†æ•°æ®
df["E"] = df["sentence"].apply(lambda x: extract_short_word_quoted_text(x) if extract_short_word_quoted_text(x) else "")

# ä¿å­˜ç»“æœ
output_path = "/content/filtered_quotes.xlsx"
df.to_excel(output_path, index=False)

print(f"âœ… å¤„ç†å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ {output_path}")