# -*- coding: utf-8 -*-
"""Reporting_verbs Диссертация.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JLDp-LZqf8mB-XiNfJFtdfxU7i0QOZJL

#CHINA+USA
import spacy
import pandas as pd

# ==================== 加载 NLP 模型 ====================
try:
    nlp = spacy.load("en_core_web_lg")  # 尝试加载更强的 `lg` 模型，提高精度
    print("✅ Loaded en_core_web_lg")
except:
    nlp = spacy.load("en_core_web_sm")  # 如果 `lg` 模型不可用，降级使用 `sm` 模型
    print("⚠️ Falling back to en_core_web_sm")

# ==================== 加载转述动词 ====================
VERB_CSV_PATH = "/content/yinyu/verbs.csv"  # 存储转述动词的 CSV 文件路径
OUTPUT_PATH = "/content/yinyu/result_CN.xlsx"  # 结果输出的 Excel 文件路径
REPORT_PATH = "/content/yinyu/report_CN.xlsx"  # 统计分析报告 Excel 文件路径

def load_reporting_verbs():
    '''从 CSV 文件加载转述动词和其对应的情感极性'''
    df = pd.read_csv(VERB_CSV_PATH)
    # 创建一个字典，键是小写的动词，值是 (原始动词, 极性)
    verbs = {row["token"].lower(): (row["token"], row["polarity"]) for _, row in df.iterrows()}
    return verbs

REPORTING_VERBS = load_reporting_verbs()

# ==================== 预定义转述短语 ====================
REPORTING_PHRASES = {
    "according to", "as per", "based on", "in the view of",
    "as reported by", "from the perspective of", "by the account of","laid out arguments"
}

# ==================== 预处理文本 ====================
def clean_and_split_text(text):
    '''清理输入文本并进行分句处理'''
    text = text.strip().replace("\n\n", ". ").replace("\n", " ").replace("  ", " ")
    doc = nlp(text)
    return [sent.text.strip() for sent in doc.sents if sent.text.strip()]

# ==================== 直接引语检测 ====================
def detect_direct_quotes(doc):
    '''检测直接引语（带引号的部分）
    逻辑：
    1. 识别文本中的引号 (`"`, `“`, `”`, `’`)，找到直接引语片段。
    2. 在引号后或引号前搜索是否存在转述动词（reporting verb）。
    3. 记录找到的转述动词，并标记为直接引语（direct）。
    '''
    quote_list = []  # 存储检测到的直接引语
    detected_direct_verbs = set()  # 记录已经检测出的转述动词
    open_quote = None  # 追踪是否开启引号

    for token in doc:
        if token.text in ['"', "“", "”", "'"] and open_quote is None:
            open_quote = token.i  # 记录引号起点
        elif token.text in ['"', "“", "”", "'"] and open_quote is not None:
            if open_quote + 1 < token.i:
                verb, polarity = None, None

                # 在引号后查找转述动词
                for i in range(token.i + 1, len(doc)):
                    if doc[i].pos_ == "VERB" and doc[i].lemma_.lower() in REPORTING_VERBS:
                        verb, polarity = REPORTING_VERBS[doc[i].lemma_.lower()]
                        detected_direct_verbs.add(verb)
                        break

                # 在引号前查找转述动词（若后面未找到）
                if verb is None:
                    for i in range(open_quote - 1, -1, -1):
                        if doc[i].pos_ == "VERB" and doc[i].lemma_.lower() in REPORTING_VERBS:
                            verb, polarity = REPORTING_VERBS[doc[i].lemma_.lower()]
                            detected_direct_verbs.add(verb)
                            break

                # 记录有效的转述动词
                if verb:
                    quote_list.append({
                        "verb": verb,
                        "polarity": polarity,
                        "quote_type": "direct"
                    })
            open_quote = None  # 关闭引号
    return quote_list, detected_direct_verbs

# ==================== 间接引语检测 ====================
def detect_indirect_quotes(doc, detected_direct_verbs):

    '''检测间接引语（不含引号的部分）
    逻辑：
    1. 识别依存关系为 `ccomp`、`xcomp` 或 `attr`，这些通常表明从句依赖于转述动词。
    2. 若该动词尚未在直接引语中被记录，则标记为间接引语（indirect）。
    3. 额外检查是否包含预定义的转述短语（如 `according to`）。
    '''

    quote_list = set()
    for word in doc:
        if word.dep_ in ("ccomp", "xcomp") or (word.dep_ == "attr" and word.head.pos_ == "VERB"):
            lemma = word.head.lemma_.lower()
            if lemma in REPORTING_VERBS and word.head.text not in detected_direct_verbs:
                verb, polarity = REPORTING_VERBS[lemma]
                quote_list.add((verb, polarity, "indirect"))
        if word.dep_ == "prep":
            phrase = f"{word.text.lower()} {doc[word.i + 1].text.lower()}" if word.i + 1 < len(doc) else word.text.lower()
            if phrase in REPORTING_PHRASES:
                quote_list.add((phrase, "neutral", "indirect"))
    return [{"verb": v, "polarity": p, "quote_type": t} for v, p, t in quote_list]

# ==================== 处理文件 ====================
def process_file(input_path):
    df = pd.read_csv(input_path)
    all_data = []

    for text in df["token"]:
        # 检查 text 是否为字符串，然后再进行处理
        if isinstance(text, str):
            sentences = clean_and_split_text(text)
            for sent in sentences:
                doc = nlp(sent)
                direct_quotes, detected_direct_verbs = detect_direct_quotes(doc)
                indirect_quotes = [] if direct_quotes else detect_indirect_quotes(doc, detected_direct_verbs)
                for res in direct_quotes + indirect_quotes:
                    all_data.append({"sentence": sent, "verb": res["verb"], "polarity": res["polarity"], "quote_type": res["quote_type"]})
        else:
            # 处理非字符串值，例如，打印警告或跳过
            print(f"警告：'token' 列中跳过非字符串值：{text}")

    # 去重处理
    df_results = pd.DataFrame(all_data).drop_duplicates()
    df_results.to_excel(OUTPUT_PATH, index=False)
    print(f"🎉 结果已保存: {OUTPUT_PATH}")

    # 生成统计报告
    generate_report(df_results)

def generate_report(df_results):
    '''生成统计报告'''
    verb_counts = df_results.groupby(["verb", "polarity"]).size().reset_index(name="count").sort_values(by="count", ascending=False)
    polarity_counts = df_results["polarity"].value_counts().reset_index()
    polarity_counts.columns = ["polarity", "count"]
    polarity_counts["percentage"] = polarity_counts["count"] / polarity_counts["count"].sum() * 100
    quote_type_counts = df_results["quote_type"].value_counts().reset_index()
    quote_type_counts.columns = ["quote_type", "count"]
    quote_type_counts["percentage"] = quote_type_counts["count"] / quote_type_counts["count"].sum() * 100

    with pd.ExcelWriter(REPORT_PATH) as writer:
        verb_counts.to_excel(writer, sheet_name="Verb Counts", index=False)
        polarity_counts.to_excel(writer, sheet_name="Polarity Distribution", index=False)
        quote_type_counts.to_excel(writer, sheet_name="Quote Type Distribution", index=False)
    print(f"📊 统计报告已保存: {REPORT_PATH}")

# 运行
INPUT_PATH = "/content/yinyu/china_yuanwen.csv"
process_file(INPUT_PATH)
"""

#CHINA+USA
import spacy
import pandas as pd

# ==================== 加载 NLP 模型 ====================
try:
    nlp = spacy.load("en_core_web_lg")  # 尝试加载更强的 `lg` 模型，提高精度
    print("✅ Loaded en_core_web_lg")
except:
    nlp = spacy.load("en_core_web_sm")  # 如果 `lg` 模型不可用，降级使用 `sm` 模型
    print("⚠️ Falling back to en_core_web_sm")

# ==================== 加载转述动词 ====================
VERB_CSV_PATH = "/content/yinyu/verbs.csv"  # 存储转述动词的 CSV 文件路径
OUTPUT_PATH = "/content/yinyu/result_CN.xlsx"  # 结果输出的 Excel 文件路径
REPORT_PATH = "/content/yinyu/report_CN.xlsx"  # 统计分析报告 Excel 文件路径

def load_reporting_verbs():
    """从 CSV 文件加载转述动词和其对应的情感极性"""
    df = pd.read_csv(VERB_CSV_PATH)
    # 创建一个字典，键是小写的动词，值是 (原始动词, 极性)
    verbs = {row["token"].lower(): (row["token"], row["polarity"]) for _, row in df.iterrows()}
    return verbs

REPORTING_VERBS = load_reporting_verbs()

# ==================== 预定义转述短语 ====================
REPORTING_PHRASES = {
    "according to", "as per", "based on", "in the view of",
    "as reported by", "from the perspective of", "by the account of","laid out arguments"
}

# ==================== 预处理文本 ====================
def clean_and_split_text(text):
    """清理输入文本并进行分句处理"""
    text = text.strip().replace("\n\n", ". ").replace("\n", " ").replace("  ", " ")
    doc = nlp(text)
    return [sent.text.strip() for sent in doc.sents if sent.text.strip()]

# ==================== 直接引语检测 ====================
def detect_direct_quotes(doc):
    """检测直接引语（带引号的部分）
    逻辑：
    1. 识别文本中的引号 (`"`, `“`, `”`, `’`)，找到直接引语片段。
    2. 在引号后或引号前搜索是否存在转述动词（reporting verb）。
    3. 记录找到的转述动词，并标记为直接引语（direct）。
    """
    quote_list = []  # 存储检测到的直接引语
    detected_direct_verbs = set()  # 记录已经检测出的转述动词
    open_quote = None  # 追踪是否开启引号

    for token in doc:
        if token.text in ['"', "“", "”", "'"] and open_quote is None:
            open_quote = token.i  # 记录引号起点
        elif token.text in ['"', "“", "”", "'"] and open_quote is not None:
            if open_quote + 1 < token.i:
                verb, polarity = None, None

                # 在引号后查找转述动词
                for i in range(token.i + 1, len(doc)):
                    if doc[i].pos_ == "VERB" and doc[i].lemma_.lower() in REPORTING_VERBS:
                        verb, polarity = REPORTING_VERBS[doc[i].lemma_.lower()]
                        detected_direct_verbs.add(verb)
                        break

                # 在引号前查找转述动词（若后面未找到）
                if verb is None:
                    for i in range(open_quote - 1, -1, -1):
                        if doc[i].pos_ == "VERB" and doc[i].lemma_.lower() in REPORTING_VERBS:
                            verb, polarity = REPORTING_VERBS[doc[i].lemma_.lower()]
                            detected_direct_verbs.add(verb)
                            break

                # 记录有效的转述动词
                if verb:
                    quote_list.append({
                        "verb": verb,
                        "polarity": polarity,
                        "quote_type": "direct"
                    })
            open_quote = None  # 关闭引号
    return quote_list, detected_direct_verbs

# ==================== 间接引语检测 ====================
def detect_indirect_quotes(doc, detected_direct_verbs):

    """检测间接引语（不含引号的部分）
    逻辑：
    1. 识别依存关系为 `ccomp`、`xcomp` 或 `attr`，这些通常表明从句依赖于转述动词。
    2. 若该动词尚未在直接引语中被记录，则标记为间接引语（indirect）。
    3. 额外检查是否包含预定义的转述短语（如 `according to`）。
    """

    quote_list = set()
    for word in doc:
        if word.dep_ in ("ccomp", "xcomp") or (word.dep_ == "attr" and word.head.pos_ == "VERB"):
            lemma = word.head.lemma_.lower()
            if lemma in REPORTING_VERBS and word.head.text not in detected_direct_verbs:
                verb, polarity = REPORTING_VERBS[lemma]
                quote_list.add((verb, polarity, "indirect"))
        if word.dep_ == "prep":
            phrase = f"{word.text.lower()} {doc[word.i + 1].text.lower()}" if word.i + 1 < len(doc) else word.text.lower()
            if phrase in REPORTING_PHRASES:
                quote_list.add((phrase, "neutral", "indirect"))
    return [{"verb": v, "polarity": p, "quote_type": t} for v, p, t in quote_list]

# ==================== 处理文件 ====================
def process_file(input_path):
    df = pd.read_csv(input_path)
    all_data = []

    for text in df["token"]:
        # 检查 text 是否为字符串，然后再进行处理
        if isinstance(text, str):
            sentences = clean_and_split_text(text)
            for sent in sentences:
                doc = nlp(sent)
                direct_quotes, detected_direct_verbs = detect_direct_quotes(doc)
                indirect_quotes = [] if direct_quotes else detect_indirect_quotes(doc, detected_direct_verbs)
                for res in direct_quotes + indirect_quotes:
                    all_data.append({"sentence": sent, "verb": res["verb"], "polarity": res["polarity"], "quote_type": res["quote_type"]})
        else:
            # 处理非字符串值，例如，打印警告或跳过
            print(f"警告：'token' 列中跳过非字符串值：{text}")

    # 去重处理
    df_results = pd.DataFrame(all_data).drop_duplicates()
    df_results.to_excel(OUTPUT_PATH, index=False)
    print(f"🎉 结果已保存: {OUTPUT_PATH}")

    # 生成统计报告
    generate_report(df_results)

def generate_report(df_results):
    """生成统计报告"""
    verb_counts = df_results.groupby(["verb", "polarity"]).size().reset_index(name="count").sort_values(by="count", ascending=False)
    polarity_counts = df_results["polarity"].value_counts().reset_index()
    polarity_counts.columns = ["polarity", "count"]
    polarity_counts["percentage"] = polarity_counts["count"] / polarity_counts["count"].sum() * 100
    quote_type_counts = df_results["quote_type"].value_counts().reset_index()
    quote_type_counts.columns = ["quote_type", "count"]
    quote_type_counts["percentage"] = quote_type_counts["count"] / quote_type_counts["count"].sum() * 100

    with pd.ExcelWriter(REPORT_PATH) as writer:
        verb_counts.to_excel(writer, sheet_name="Verb Counts", index=False)
        polarity_counts.to_excel(writer, sheet_name="Polarity Distribution", index=False)
        quote_type_counts.to_excel(writer, sheet_name="Quote Type Distribution", index=False)
    print(f"📊 统计报告已保存: {REPORT_PATH}")

# 运行
INPUT_PATH = "/content/yinyu/china_yuanwen.csv"
process_file(INPUT_PATH)

import re
import pandas as pd
import pymorphy2
from tqdm import tqdm

# 初始化 pymorphy2 词形还原器
morph = pymorphy2.MorphAnalyzer()

# ==================== 加载转述动词 ====================
VERB_CSV_PATH = "/Users/chenxiaoting/Downloads/我的文件/博士学习/大论文/论文数据/yinyu/verbs_ru.csv"  # 动词词库路径（包含原形）


def load_reporting_verbs():
    """从 CSV 文件加载转述动词和其对应的情感极性（原形）"""
    df = pd.read_csv(VERB_CSV_PATH)
    verbs = {row["token"].lower(): (row["token"], row["polarity"]) for _, row in df.iterrows()}
    return verbs


REPORTING_VERBS = load_reporting_verbs()

# ==================== 预定义转述短语 ====================
REPORTING_PHRASES = {
    "по словам", "по его словам", "по ее словам", "по их словам",
    "по мнению", "по его мнению", "по ее мнению", "по их мнению",
    "по оценкам", "по информации", "по данным", "по сообщениям",
    "по сведениям", "по версии", "по предварительной информации",
    "по неофициальным данным", "по слухам", "по утверждению", "по оценке",
    "со ссылкой на", "как отмечать", "как заявлять", "как писать",
    "как утверждать", "как подчеркнуть", "как пояснять", "как сообщить",
    "как подтвердить", "как объявить", "как заверить", "как предостеречь",
    "как осудить", "как опровергнуть", "как выступить против",
    "как раскритиковать", "как ожидаться", "как прогнозировать",
    "как предупредить", "как угрожать", "как предсказать", "как спорить",
    "как дискутирует", "как доказывает", "как возражает", "как опровергает",
    "как раскритиковал", "как обсуждается", "как настаивает",
    "сходятся во мнении", "обратить внимание", "обращаться внимание"
}


# ==================== 动词词形还原 ====================
def lemmatize_verb(verb):
    """使用 pymorphy2 进行词形还原"""
    parsed = morph.parse(verb)[0]
    return parsed.normal_form  # 返回词形还原后的动词原形


def get_base_verb(verb):
    """获取动词的原形和极性"""
    lemma = lemmatize_verb(verb.lower())
    if lemma in REPORTING_VERBS:
        return lemma, REPORTING_VERBS[lemma][1]  # 返回原形和极性
    return verb, "未知"  # 无法还原时返回原词


# ==================== 直接引语检测（正则） ====================
DIRECT_SPEECH_PATTERNS = [
    r'["«„](.*?)["»“]',  # 引号内的文本
    r'—\s+(.*)',  # 破折号引出的文本
    r'(\w+):\s+["«„](.*?)["»“]'  # 冒号 + 引号
]


def detect_direct_quotes(text):
    """检测直接引语"""
    direct_quotes = []
    detected_direct_verbs = set()

    for pattern in DIRECT_SPEECH_PATTERNS:
        matches = re.findall(pattern, text)
        if matches:
            for match in matches:
                # 提取引语内容并检查转述动词
                quote = match[0] if isinstance(match, tuple) else match
                for token in re.findall(r'\w+', text):
                    lemma, polarity = get_base_verb(token)
                    if lemma in REPORTING_VERBS:
                        detected_direct_verbs.add(lemma)
                        direct_quotes.append({
                            "sentence": text,
                            "verb": token,
                            "lemma": lemma,
                            "polarity": polarity,
                            "quote_type": "direct"
                        })
    return direct_quotes, detected_direct_verbs


# ==================== 间接引语检测 ====================
def detect_indirect_quotes(text, detected_direct_verbs):
    """检测间接引语（要求包含 'что' 和转述动词/短语）"""
    indirect_quotes = []
    text_lower = text.lower()

    if 'что' in text_lower:
        # 检查转述动词
        for token in re.findall(r'\w+', text):
            lemma, polarity = get_base_verb(token)
            if lemma in REPORTING_VERBS and lemma not in detected_direct_verbs:
                indirect_quotes.append({
                    "sentence": text,
                    "verb": token,
                    "lemma": lemma,
                    "polarity": polarity,
                    "quote_type": "indirect"
                })
                break
        # 检查转述短语
        for phrase in REPORTING_PHRASES:
            if phrase in text_lower:
                indirect_quotes.append({
                    "sentence": text,
                    "verb": phrase,
                    "lemma": phrase,
                    "polarity": "中性",  # 短语的极性需单独定义（此处简化）
                    "quote_type": "indirect"
                })
                break
    return indirect_quotes


# ==================== 分句处理 ====================
def split_into_sentences(text):
    """将文本分割成句子"""
    return re.split(r'(?<=[.!?])\s+', text)


# ==================== 统计报告生成 ====================
def generate_report(df_results):
    """生成统计报告"""
    verb_counts = df_results.groupby(["verb", "polarity"]).size().reset_index(name="count").sort_values(by="count",
                                                                                                        ascending=False)
    # 统计转述短语
    phrase_counts = df_results[df_results['quote_type'] == 'indirect'].groupby(["verb", "polarity"]).size().reset_index(
        name="count").sort_values(by="count", ascending=False)

    # 输出动词和短语统计
    print(f"Verb counts: {verb_counts.shape[0]}")  # 打印动词统计条数
    print(f"Phrase counts: {phrase_counts.shape[0]}")  # 打印短语统计条数

    polarity_counts = df_results["polarity"].value_counts().reset_index()
    polarity_counts.columns = ["polarity", "count"]
    polarity_counts["percentage"] = polarity_counts["count"] / polarity_counts["count"].sum() * 100
    quote_type_counts = df_results["quote_type"].value_counts().reset_index()
    quote_type_counts.columns = ["quote_type", "count"]
    quote_type_counts["percentage"] = quote_type_counts["count"] / quote_type_counts["count"].sum() * 100

    with pd.ExcelWriter("/Users/chenxiaoting/Downloads/我的文件/博士学习/大论文/论文数据/yinyu/report_RU.xlsx") as writer:
        verb_counts.to_excel(writer, sheet_name="Verb Counts", index=False)
        phrase_counts.to_excel(writer, sheet_name="Phrase Counts", index=False)  # 添加短语统计
        polarity_counts.to_excel(writer, sheet_name="Polarity Distribution", index=False)
        quote_type_counts.to_excel(writer, sheet_name="Quote Type Distribution", index=False)

    print(f"📊 统计报告已保存 report_RU.xlsx")


# ==================== 主处理流程 ====================
def process_file(input_path):
    df = pd.read_csv(input_path)
    all_data = []

    for text in tqdm(df["token"], desc="Processing", unit="text"):
        if isinstance(text, str):
            sentences = split_into_sentences(text)
            for sentence in sentences:
                # 直接引语检测
                direct_quotes, detected_verbs = detect_direct_quotes(sentence)
                # 间接引语检测（仅当无直接引语时）
                indirect_quotes = detect_indirect_quotes(sentence, detected_verbs) if not direct_quotes else []
                all_data.extend(direct_quotes + indirect_quotes)

    # 保存结果（去除重复项）
    df_results = pd.DataFrame(all_data).drop_duplicates()
    df_results.to_excel("/Users/chenxiaoting/Downloads/我的文件/博士学习/大论文/论文数据/yinyu/result_RU.xlsx", index=False)
    print("🎉 结果已保存 result_RU.xlsx")


# 🚀 运行
process_file("/Users/chenxiaoting/Downloads/我的文件/博士学习/大论文/论文数据/yinyu/file_yuan/RU_yuanwen.csv")

#找出每行中引号内字符数小于3的引用内容
import re
import pandas as pd

# 读取文件
file_path = "/content/result_RU.xlsx"
df = pd.read_excel(file_path, usecols=["sentence"])  # 只读取第一列 "sentence"

# 正则表达式匹配双引号（包括俄语双引号）内的内容
QUOTE_PATTERN = r'["]([^"]*?)["]'  # 兼容俄语双引号

def clean_text(text):
    """去除标点符号，防止影响单词计数"""
    return re.sub(r'[^\w\s]', '', text)

def extract_short_word_quoted_text(text):
    """查找所有引号内单词数小于等于 3 的文本"""
    matches = re.findall(QUOTE_PATTERN, text)  # 查找所有引号内的内容
    short_words = [match for match in matches if len(clean_text(match).split()) <= 3]  # 过滤单词数 ≤ 3
    return ", ".join(short_words) if short_words else ""

# 处理数据
df["E"] = df["sentence"].apply(lambda x: extract_short_word_quoted_text(x) if extract_short_word_quoted_text(x) else "")

# 保存结果
output_path = "/content/filtered_quotes.xlsx"
df.to_excel(output_path, index=False)

print(f"✅ 处理完成，结果已保存至 {output_path}")